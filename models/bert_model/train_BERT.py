import pandas as pd
import numpy as np
import torch
import re
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.optim import AdamW
from transformers import (
    BertTokenizer, 
    BertForSequenceClassification,
    get_linear_schedule_with_warmup
)
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report
import random
import time
import warnings
import os
import json
from tqdm import tqdm
import matplotlib
from collections import Counter

# è®¾ç½®ä¸­æ–‡å­—ä½“
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'STXihei']
matplotlib.rcParams['axes.unicode_minus'] = False

warnings.filterwarnings('ignore')

def print_gpu_memory_usage(device=None):
    """æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µ"""
    if torch.cuda.is_available():
        if device is None:
            device = torch.cuda.current_device()
        
        allocated = torch.cuda.memory_allocated(device) / 1024**2
        cached = torch.cuda.memory_reserved(device) / 1024**2
        
        print(f"GPUå†…å­˜ä½¿ç”¨: {allocated:.2f} MB (å·²åˆ†é…) / {cached:.2f} MB (ç¼“å­˜)")
        return allocated, cached
    else:
        print("æœªæ£€æµ‹åˆ°GPU")
        return 0, 0

def clear_gpu_cache():
    """æ¸…ç©ºGPUç¼“å­˜"""
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print("å·²æ¸…ç©ºGPUç¼“å­˜")

# ==================== 1. é…ç½®å‚æ•° ====================
class Config:
    # æ•°æ®è·¯å¾„
    data_dir = "data"
    
    # æ¨¡å‹å‚æ•°
    model_path = "bert-base-chinese"  # ä½¿ç”¨å®˜æ–¹é¢„è®­ç»ƒæ¨¡å‹
    max_length = 128
    num_labels = 2
    
    # è®­ç»ƒå‚æ•°
    batch_size = 16
    num_epochs = 10  # å‡å°‘epochæ•°ä»¥åŠ å¿«è®­ç»ƒ
    learning_rate = 2e-5
    warmup_ratio = 0.1
    weight_decay = 0.01
    patience = 3  # å‡å°‘æ—©åœè€å¿ƒå€¼
    
    # äº¤å‰éªŒè¯
    n_folds = 5
    
    # è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # éšæœºç§å­
    seed = 42
    
    # å¯è§†åŒ–ç›®å½•
    viz_dir = 'bert_visualizations'
    
    def set_seed(self):
        random.seed(self.seed)
        np.random.seed(self.seed)
        torch.manual_seed(self.seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed_all(self.seed)

config = Config()
config.set_seed()

# åˆ›å»ºå¯è§†åŒ–ç›®å½•
os.makedirs(config.viz_dir, exist_ok=True)

# ==================== 2. æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç† ====================
def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    
    text = str(text).strip()
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?;\'"ã€]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def clean_data(df, text_col='sentence', label_col='label'):
    """æ¸…æ´—æ•°æ®"""
    print(f"åŸå§‹æ•°æ®å¤§å°: {len(df)}")
    
    # å¤‡ä»½åŸå§‹æ•°æ®
    df_original = df.copy()
    
    # å¤„ç†NaNå€¼
    df = df.dropna(subset=[text_col, label_col]).copy()
    
    # æ¸…ç†æ–‡æœ¬
    df[text_col] = df[text_col].apply(clean_text)
    
    # ç¡®ä¿æ ‡ç­¾ä¸ºæ•´æ•°
    df[label_col] = pd.to_numeric(df[label_col], errors='coerce')
    df = df.dropna(subset=[label_col])
    df[label_col] = df[label_col].astype(int)
    
    # ç§»é™¤ç©ºæ–‡æœ¬
    df = df[df[text_col].str.len() > 0].copy()
    
    print(f"æ¸…æ´—åæ•°æ®å¤§å°: {len(df)}")
    print(f"ç§»é™¤çš„è¡Œæ•°: {len(df_original) - len(df)}")
    
    return df

# ==================== 3. æ•°æ®é›†ç±» ====================
class BERTDataset(Dataset):
    def __init__(self, texts, labels, tokenizer, max_length):
        self.texts = texts
        self.labels = labels
        self.tokenizer = tokenizer
        self.max_length = max_length
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        text = str(self.texts[idx])
        label = int(self.labels[idx])
        
        encoding = self.tokenizer.encode_plus(
            text,
            truncation=True,
            padding='max_length',
            max_length=self.max_length,
            return_tensors='pt'
        )
        
        item = {
            'input_ids': encoding['input_ids'].flatten(),
            'attention_mask': encoding['attention_mask'].flatten(),
            'labels': torch.tensor(label, dtype=torch.long)
        }
        
        if 'token_type_ids' in encoding:
            item['token_type_ids'] = encoding['token_type_ids'].flatten()
        
        return item

# ==================== 4. å¯è§†åŒ–å‡½æ•° ====================
def plot_training_history(history, fold_idx, save_dir=config.viz_dir):
    """ç»˜åˆ¶å•æŠ˜è®­ç»ƒå†å²"""
    os.makedirs(save_dir, exist_ok=True)
    
    epochs = range(1, len(history['train_loss']) + 1)
    
    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle(f'BERTæ¨¡å‹ - ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå†å²', fontsize=16, fontweight='bold')
    
    # æŸå¤±æ›²çº¿
    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('æŸå¤±')
    axes[0, 0].set_title('æŸå¤±æ›²çº¿')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)
    
    # å‡†ç¡®ç‡æ›²çº¿
    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('å‡†ç¡®ç‡')
    axes[0, 1].set_title('å‡†ç¡®ç‡æ›²çº¿')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)
    
    # F1-scoreæ›²çº¿
    axes[0, 2].plot(epochs, history['train_f1'], 'b-', label='è®­ç»ƒF1', linewidth=2)
    axes[0, 2].plot(epochs, history['val_f1'], 'r-', label='éªŒè¯F1', linewidth=2)
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('F1-score')
    axes[0, 2].set_title('F1-scoreæ›²çº¿')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)
    
    # ç²¾ç¡®ç‡æ›²çº¿
    axes[1, 0].plot(epochs, history['train_precision'], 'b-', label='è®­ç»ƒç²¾ç¡®ç‡', linewidth=2)
    axes[1, 0].plot(epochs, history['val_precision'], 'r-', label='éªŒè¯ç²¾ç¡®ç‡', linewidth=2)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('ç²¾ç¡®ç‡')
    axes[1, 0].set_title('ç²¾ç¡®ç‡æ›²çº¿')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)
    
    # å¬å›ç‡æ›²çº¿
    axes[1, 1].plot(epochs, history['train_recall'], 'b-', label='è®­ç»ƒå¬å›ç‡', linewidth=2)
    axes[1, 1].plot(epochs, history['val_recall'], 'r-', label='éªŒè¯å¬å›ç‡', linewidth=2)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('å¬å›ç‡')
    axes[1, 1].set_title('å¬å›ç‡æ›²çº¿')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)
    
    # å­¦ä¹ ç‡æ›²çº¿
    if 'learning_rate' in history:
        axes[1, 2].plot(epochs, history['learning_rate'], 'g-', label='å­¦ä¹ ç‡', linewidth=2)
        axes[1, 2].set_xlabel('Epoch')
        axes[1, 2].set_ylabel('å­¦ä¹ ç‡')
        axes[1, 2].set_title('å­¦ä¹ ç‡å˜åŒ–')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
    else:
        axes[1, 2].axis('off')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'bert_training_history_fold_{fold_idx}.png'), 
                dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ BERTç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå†å²å›¾å·²ä¿å­˜")

def plot_confusion_matrix_comprehensive(y_true, y_pred, fold_idx=None, model_name=None, save_dir=config.viz_dir):
    """ç»˜åˆ¶å®Œæ•´çš„æ··æ·†çŸ©é˜µ"""
    os.makedirs(save_dir, exist_ok=True)
    
    cm = confusion_matrix(y_true, y_pred)
    classes = ['è´Ÿé¢', 'æ­£é¢']
    
    fig, axes = plt.subplots(1, 2, figsize=(12, 5))
    
    # 1. çƒ­åŠ›å›¾æ··æ·†çŸ©é˜µ
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                xticklabels=classes, yticklabels=classes, ax=axes[0])
    axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
    axes[0].set_ylabel('çœŸå®æ ‡ç­¾')
    title = 'æ··æ·†çŸ©é˜µ'
    if fold_idx is not None:
        title += f' (ç¬¬{fold_idx+1}æŠ˜)'
    if model_name:
        title += f' ({model_name})'
    axes[0].set_title(title)
    
    # 2. ç™¾åˆ†æ¯”æ··æ·†çŸ©é˜µ
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Oranges',
                xticklabels=classes, yticklabels=classes, ax=axes[1])
    axes[1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
    axes[1].set_ylabel('çœŸå®æ ‡ç­¾')
    axes[1].set_title('æ··æ·†çŸ©é˜µï¼ˆç™¾åˆ†æ¯”ï¼‰')
    
    plt.tight_layout()
    filename = 'bert_confusion_matrix'
    if fold_idx is not None:
        filename += f'_fold_{fold_idx}'
    if model_name:
        filename += f'_{model_name}'
    filename += '.png'
    
    plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ BERTæ··æ·†çŸ©é˜µå·²ä¿å­˜: {filename}")

# ==================== 5. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° ====================
def train_epoch_bert(model, dataloader, optimizer, scheduler, device, class_weights=None):
    """BERTè®­ç»ƒä¸€ä¸ªepoch"""
    model.train()
    total_loss = 0
    all_predictions = []
    all_labels = []
    
    pbar = tqdm(dataloader, desc='è®­ç»ƒ', leave=False)
    
    for batch in pbar:
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)
        
        # å‰å‘ä¼ æ’­
        outputs = model(
            input_ids=input_ids,
            attention_mask=attention_mask,
            labels=labels
        )
        
        loss = outputs.loss
        
        # å¦‚æœæœ‰ç±»åˆ«æƒé‡ï¼Œè°ƒæ•´æŸå¤±
        if class_weights is not None:
            weights = torch.tensor(class_weights, dtype=torch.float).to(device)
            loss_fct = nn.CrossEntropyLoss(weight=weights)
            logits = outputs.logits
            loss = loss_fct(logits.view(-1, config.num_labels), labels.view(-1))
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        optimizer.step()
        scheduler.step()
        
        # è®°å½•
        total_loss += loss.item()
        preds = torch.argmax(outputs.logits, dim=1).detach().cpu().numpy()
        all_predictions.extend(preds)
        all_labels.extend(labels.cpu().numpy())
    
    # è®¡ç®—epochæŒ‡æ ‡
    epoch_loss = total_loss / len(dataloader)
    epoch_acc = accuracy_score(all_labels, all_predictions)
    epoch_f1 = f1_score(all_labels, all_predictions, average='macro')
    epoch_precision = precision_score(all_labels, all_predictions, average='macro')
    epoch_recall = recall_score(all_labels, all_predictions, average='macro')
    
    return {
        'loss': epoch_loss,
        'accuracy': epoch_acc,
        'f1': epoch_f1,
        'precision': epoch_precision,
        'recall': epoch_recall
    }

def evaluate_epoch_bert(model, dataloader, device):
    """BERTè¯„ä¼°ä¸€ä¸ªepoch"""
    model.eval()
    total_loss = 0
    all_predictions = []
    all_labels = []
    all_probs = []
    
    with torch.no_grad():
        for batch in tqdm(dataloader, desc='è¯„ä¼°', leave=False):
            input_ids = batch['input_ids'].to(device)
            attention_mask = batch['attention_mask'].to(device)
            labels = batch['labels'].to(device)
            
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels
            )
            
            loss = outputs.loss
            logits = outputs.logits
            probs = torch.softmax(logits, dim=1)
            
            total_loss += loss.item()
            preds = torch.argmax(logits, dim=1).cpu().numpy()
            
            all_predictions.extend(preds)
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())
    
    # è®¡ç®—epochæŒ‡æ ‡
    epoch_loss = total_loss / len(dataloader)
    epoch_acc = accuracy_score(all_labels, all_predictions)
    epoch_f1 = f1_score(all_labels, all_predictions, average='macro')
    epoch_precision = precision_score(all_labels, all_predictions, average='macro')
    epoch_recall = recall_score(all_labels, all_predictions, average='macro')
    
    return {
        'loss': epoch_loss,
        'accuracy': epoch_acc,
        'f1': epoch_f1,
        'precision': epoch_precision,
        'recall': epoch_recall,
        'predictions': np.array(all_predictions),
        'labels': np.array(all_labels),
        'probabilities': np.array(all_probs)
    }

# ==================== 6. å•æŠ˜è®­ç»ƒå‡½æ•° ====================
def train_single_fold_bert(fold_idx, train_df, val_df, tokenizer, device, n_epochs=10):
    """è®­ç»ƒå•ä¸ªæŠ˜çš„BERTæ¨¡å‹"""
    print(f"\n{'='*70}")
    print(f"ğŸ“Š BERTæ¨¡å‹ - ç¬¬ {fold_idx+1}/5 æŠ˜è®­ç»ƒ")
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
    print(f"{'='*70}")
    
    # è®¾ç½®GPUä¼˜åŒ–
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        print(f"GPUå†…å­˜ä½¿ç”¨æƒ…å†µ: {torch.cuda.memory_allocated(device)/1024**2:.2f} MB")
    
    # åˆ›å»ºæ•°æ®é›†
    train_dataset = BERTDataset(train_df['sentence'].values, train_df['label'].values, tokenizer, config.max_length)
    val_dataset = BERTDataset(val_df['sentence'].values, val_df['label'].values, tokenizer, config.max_length)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=config.batch_size, shuffle=False)
    
    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºBERTæ¨¡å‹ä¸­...")
    model = BertForSequenceClassification.from_pretrained(
        config.model_path,
        num_labels=config.num_labels
    )
    
    # å°†æ¨¡å‹ç§»åŠ¨åˆ°è®¾å¤‡
    model.to(device)
    
    # å¦‚æœæ˜¯å¤šGPUè®­ç»ƒ
    if torch.cuda.device_count() > 1:
        print(f"ä½¿ç”¨ {torch.cuda.device_count()} ä¸ªGPUè¿›è¡Œè®­ç»ƒ")
        model = nn.DataParallel(model)
    
    # è®¡ç®—ç±»åˆ«æƒé‡
    class_counts = Counter(train_df['label'].values)
    total_samples = len(train_df)
    class_weights = [total_samples / (len(class_counts) * count) for count in class_counts.values()]
    
    # åˆ›å»ºä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨
    optimizer = AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)
    total_steps = len(train_loader) * n_epochs
    scheduler = get_linear_schedule_with_warmup(
        optimizer,
        num_warmup_steps=int(total_steps * config.warmup_ratio),
        num_training_steps=total_steps
    )
    
    # è®­ç»ƒå†å²è®°å½•
    history = {
        'train_loss': [],
        'train_acc': [],
        'train_f1': [],
        'train_precision': [],
        'train_recall': [],
        'val_loss': [],
        'val_acc': [],
        'val_f1': [],
        'val_precision': [],
        'val_recall': [],
        'learning_rate': []
    }
    
    # è®­ç»ƒå¾ªç¯
    print("å¼€å§‹è®­ç»ƒ...")
    best_val_loss = float('inf')
    best_val_acc = 0
    best_val_f1 = 0
    patience_counter = 0
    best_epoch = 0
    best_model_state = None
    
    for epoch in range(n_epochs):
        print(f"\nEpoch {epoch+1}/{n_epochs}")
        
        # è®­ç»ƒ
        train_metrics = train_epoch_bert(model, train_loader, optimizer, scheduler, device, class_weights)
        
        # éªŒè¯
        val_metrics = evaluate_epoch_bert(model, val_loader, device)
        
        # è®°å½•å†å²
        history['train_loss'].append(train_metrics['loss'])
        history['train_acc'].append(train_metrics['accuracy'])
        history['train_f1'].append(train_metrics['f1'])
        history['train_precision'].append(train_metrics['precision'])
        history['train_recall'].append(train_metrics['recall'])
        history['val_loss'].append(val_metrics['loss'])
        history['val_acc'].append(val_metrics['accuracy'])
        history['val_f1'].append(val_metrics['f1'])
        history['val_precision'].append(val_metrics['precision'])
        history['val_recall'].append(val_metrics['recall'])
        history['learning_rate'].append(scheduler.get_last_lr()[0])
        
        # æ‰“å°è¿›åº¦
        print(f"  è®­ç»ƒ: Loss={train_metrics['loss']:.4f}, Acc={train_metrics['accuracy']*100:.2f}%, F1={train_metrics['f1']*100:.2f}%")
        print(f"  éªŒè¯: Loss={val_metrics['loss']:.4f}, Acc={val_metrics['accuracy']*100:.2f}%, F1={val_metrics['f1']*100:.2f}%")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['loss'] < best_val_loss:
            best_val_loss = val_metrics['loss']
            best_val_acc = val_metrics['accuracy']
            best_val_f1 = val_metrics['f1']
            best_epoch = epoch
            patience_counter = 0
            
            # ä¿å­˜æ¨¡å‹çŠ¶æ€ï¼ˆå¦‚æœæ˜¯DataParallelï¼Œéœ€è¦ä¿å­˜module.state_dictï¼‰
            if isinstance(model, nn.DataParallel):
                best_model_state = model.module.state_dict().copy()
            else:
                best_model_state = model.state_dict().copy()
            
            best_val_predictions = val_metrics['predictions']
            best_val_labels = val_metrics['labels']
            best_val_probs = val_metrics['probabilities']
            
            print(f"  âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ (val_acc: {val_metrics['accuracy']*100:.2f}%, val_f1: {val_metrics['f1']*100:.2f}%)")
        else:
            patience_counter += 1
            print(f"  éªŒè¯æŸå¤±æœªæ”¹å–„ ({patience_counter}/{config.patience})")
        
        # æ—©åœ
        if patience_counter >= config.patience:
            print(f"  â° æ—©åœè§¦å‘ï¼Œåœ¨ç¬¬{epoch+1}è½®åœæ­¢è®­ç»ƒ")
            break
        
        # æ‰“å°GPUå†…å­˜ä½¿ç”¨æƒ…å†µï¼ˆå¦‚æœä½¿ç”¨GPUï¼‰
        if torch.cuda.is_available():
            print(f"  GPUå†…å­˜: {torch.cuda.memory_allocated(device)/1024**2:.2f} MB")
    
    # ä¿å­˜æœ€ä½³æ¨¡å‹
    model_save_path = f'bert_fold_{fold_idx}_best.pth'
    torch.save(best_model_state, model_save_path)
    print(f"âœ“ æœ€ä½³æ¨¡å‹å·²ä¿å­˜åˆ° {model_save_path}")
    
    # ä¿å­˜transformersæ ¼å¼çš„æ¨¡å‹
    transformers_save_path = f'bert_fold_{fold_idx}_best_transformers'
    if isinstance(model, nn.DataParallel):
        model.module.save_pretrained(transformers_save_path)
    else:
        model.save_pretrained(transformers_save_path)
    tokenizer.save_pretrained(transformers_save_path)
    print(f"âœ“ Transformersæ ¼å¼æ¨¡å‹å·²ä¿å­˜åˆ° {transformers_save_path}")
    
    # ç»˜åˆ¶è®­ç»ƒå†å²
    plot_training_history(history, fold_idx)
    
    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    plot_confusion_matrix_comprehensive(best_val_labels, best_val_predictions, fold_idx)
    
    return {
        'fold': fold_idx,
        'model_config': {
            'model_path': config.model_path,
            'num_labels': config.num_labels,
            'max_length': config.max_length
        },
        'tokenizer': tokenizer,
        'best_val_loss': best_val_loss,
        'best_val_acc': best_val_acc,
        'best_val_f1': best_val_f1,
        'history': history,
        'val_predictions': best_val_predictions,
        'val_labels': best_val_labels,
        'val_probabilities': best_val_probs,
        'model_path': model_save_path,
        'transformers_path': transformers_save_path,
        'best_epoch': best_epoch
    }

# ==================== 7. åŠ è½½äº¤å‰éªŒè¯æ•°æ® ====================
def load_cross_validation_data(data_dir="data"):
    """åŠ è½½5æŠ˜äº¤å‰éªŒè¯æ•°æ®"""
    print("åŠ è½½äº¤å‰éªŒè¯æ•°æ®...")
    
    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„åˆ†å¥½çš„æŠ˜æ–‡ä»¶
    pre_split_files_exist = True
    for fold_idx in range(5):
        train_path = os.path.join(data_dir, f"train_fold_{fold_idx}.csv")
        val_path = os.path.join(data_dir, f"val_fold_{fold_idx}.csv")
        if not (os.path.exists(train_path) and os.path.exists(val_path)):
            pre_split_files_exist = False
            break
    
    folds = []
    
    if pre_split_files_exist:
        print("ğŸ“ ä½¿ç”¨é¢„åˆ†å¥½çš„äº¤å‰éªŒè¯æ•°æ®")
        for fold_idx in range(5):
            train_path = os.path.join(data_dir, f"train_fold_{fold_idx}.csv")
            val_path = os.path.join(data_dir, f"val_fold_{fold_idx}.csv")
            
            train_df = pd.read_csv(train_path)
            val_df = pd.read_csv(val_path)
            
            # æ¸…æ´—æ•°æ®
            train_df = clean_data(train_df)
            val_df = clean_data(val_df)
            
            folds.append({
                'fold': fold_idx,
                'train': train_df,
                'val': val_df
            })
            
            print(f"  ç¬¬{fold_idx+1}æŠ˜: è®­ç»ƒé›†={len(train_df)}, éªŒè¯é›†={len(val_df)}")
    else:
        print("ğŸ“ ä» train.csv åˆ›å»ºäº¤å‰éªŒè¯æŠ˜")
        # åŠ è½½ä¸»è®­ç»ƒæ•°æ®
        train_path = os.path.join(data_dir, "train.csv")
        if not os.path.exists(train_path):
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶: {train_path}")
        
        train_df = pd.read_csv(train_path)
        print(f"  è®­ç»ƒé›†æ€»å¤§å°: {len(train_df)} è¡Œ")
        
        # æ¸…æ´—æ•°æ®
        train_df = clean_data(train_df)
        print(f"  æœ‰æ•ˆæ•°æ®: {len(train_df)} è¡Œ")
        
        # åˆ›å»ºäº¤å‰éªŒè¯æŠ˜
        from sklearn.model_selection import KFold
        kf = KFold(n_splits=5, shuffle=True, random_state=42)
        
        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(train_df)):
            train_fold = train_df.iloc[train_idx].copy()
            val_fold = train_df.iloc[val_idx].copy()
            
            print(f"  ç¬¬{fold_idx}æŠ˜: è®­ç»ƒé›† {len(train_fold)} è¡Œ, éªŒè¯é›† {len(val_fold)} è¡Œ")
            
            folds.append({
                'fold': fold_idx,
                'train': train_fold,
                'val': val_fold
            })
    
    return folds

# ==================== 8. ä¸»è®­ç»ƒæµç¨‹ï¼ˆ5æŠ˜äº¤å‰éªŒè¯ï¼‰ ====================
def main_bert_cross_validation():
    """BERTæ¨¡å‹5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ"""
    print("=" * 70)
    print("ğŸ¤– BERTæƒ…æ„Ÿåˆ†ææ¨¡å‹ - 5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ")
    print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {config.device}")
    if torch.cuda.is_available():
        print(f"ğŸ® GPUå‹å·: {torch.cuda.get_device_name(0)}")
        print(f"ğŸ® GPUæ•°é‡: {torch.cuda.device_count()}")
    print("=" * 70)
    
    # 1. åŠ è½½æ•°æ®
    print("\n" + "="*50)
    print("ğŸ“‚ 1. åŠ è½½æ•°æ®")
    print("="*50)
    
    folds_data = load_cross_validation_data()
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    test_data_path = os.path.join(config.data_dir, "dev.csv")
    if os.path.exists(test_data_path):
        test_df = pd.read_csv(test_data_path)
        test_df = clean_data(test_df)
        print(f"æµ‹è¯•é›†å¤§å°: {len(test_df)}")
    else:
        print("è­¦å‘Š: æœªæ‰¾åˆ°æµ‹è¯•æ•°æ®æ–‡ä»¶ï¼Œå°†ä½¿ç”¨éªŒè¯é›†è¿›è¡Œè¯„ä¼°")
        test_df = folds_data[0]['val']  # ä½¿ç”¨ç¬¬ä¸€æŠ˜çš„éªŒè¯é›†ä½œä¸ºæµ‹è¯•é›†
    
    # 2. åŠ è½½tokenizer
    print("\n" + "="*50)
    print("ğŸ”¤ 2. åŠ è½½Tokenizer")
    print("="*50)
    
    print(f"åŠ è½½BERT tokenizer: {config.model_path}")
    tokenizer = BertTokenizer.from_pretrained(config.model_path)
    
    # 3. åˆ›å»ºæµ‹è¯•æ•°æ®é›†
    print("\n" + "="*50)
    print("ğŸ§ª 3. åˆ›å»ºæµ‹è¯•æ•°æ®é›†")
    print("="*50)
    
    test_dataset = BERTDataset(test_df['sentence'].values, test_df['label'].values, tokenizer, config.max_length)
    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False)
    
    # 4. è®­ç»ƒæ‰€æœ‰æŠ˜
    print("\n" + "="*50)
    print("ğŸ‹ï¸ 4. è®­ç»ƒæ‰€æœ‰æŠ˜")
    print("="*50)
    
    all_model_states = []
    all_model_configs = []
    all_fold_results = []
    
    for fold_idx, fold_data in enumerate(folds_data):
        try:
            # è®­ç»ƒå•æŠ˜
            fold_result = train_single_fold_bert(
                fold_idx, 
                fold_data['train'], 
                fold_data['val'], 
                tokenizer, 
                config.device,
                n_epochs=config.num_epochs
            )
            
            # æ”¶é›†ç»“æœ
            all_fold_results.append(fold_result)
            
            # åŠ è½½æœ€ä½³æ¨¡å‹çŠ¶æ€
            best_state = torch.load(fold_result['model_path'], map_location='cpu')
            all_model_states.append(best_state)
            all_model_configs.append(fold_result['model_config'])
            
            print(f"âœ“ ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå®Œæˆ")
            
        except Exception as e:
            print(f"âŒ ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå¤±è´¥: {e}")
            continue
    
    # 5. è¯„ä¼°å•ä¸ªæ¨¡å‹å’Œé›†æˆæ¨¡å‹
    print("\n" + "="*70)
    print("ğŸ“ˆ 5. è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    print("="*70)
    
    if not all_model_states:
        raise ValueError("æ²¡æœ‰æˆåŠŸè®­ç»ƒçš„æ¨¡å‹ï¼Œæ— æ³•è¿›è¡Œè¯„ä¼°")
    
    # é¢„å…ˆåŠ è½½æ‰€æœ‰æ¨¡å‹åˆ°GPU
    print("\né¢„å…ˆåŠ è½½æ‰€æœ‰æ¨¡å‹åˆ°å†…å­˜...")
    loaded_models = []
    for i, (state_dict, model_config) in enumerate(zip(all_model_states, all_model_configs)):
        print(f"  åŠ è½½ç¬¬{i+1}ä¸ªæ¨¡å‹...")
        model = BertForSequenceClassification.from_pretrained(
            model_config['model_path'],
            num_labels=model_config['num_labels']
        )
        model.load_state_dict(state_dict)
        model.to(config.device)
        model.eval()
        loaded_models.append(model)
    
    print("âœ“ æ‰€æœ‰æ¨¡å‹åŠ è½½å®Œæˆ")
    
    # è¯„ä¼°å•ä¸ªæ¨¡å‹ï¼ˆç¬¬ä¸€æŠ˜ï¼‰
    print("\nè¯„ä¼°å•ä¸ªæ¨¡å‹ï¼ˆç¬¬ä¸€æŠ˜ï¼‰...")
    single_model = loaded_models[0]
    single_test_metrics = evaluate_epoch_bert(single_model, test_loader, config.device)
    single_test_acc = single_test_metrics['accuracy']
    single_test_f1 = single_test_metrics['f1']
    
    print(f"å•æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {single_test_acc*100:.2f}%")
    print(f"å•æ¨¡å‹æµ‹è¯•F1-score: {single_test_f1*100:.2f}%")
    
    # è¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰
    print("\nè¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰...")
    soft_voting_acc = 0
    total_samples = 0
    all_soft_predictions = []
    all_soft_probs = []
    all_test_labels = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="é›†æˆè¯„ä¼°ï¼ˆè½¯æŠ•ç¥¨ï¼‰"):
            input_ids = batch['input_ids'].to(config.device)
            attention_mask = batch['attention_mask'].to(config.device)
            labels = batch['labels'].cpu().numpy()
            
            # é›†æˆé¢„æµ‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰- ä½¿ç”¨é¢„å…ˆåŠ è½½çš„æ¨¡å‹
            all_probs = []
            for model in loaded_models:
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                probs = torch.softmax(outputs.logits, dim=1)
                all_probs.append(probs)
            
            # å¹³å‡æ¦‚ç‡
            avg_probs = torch.mean(torch.stack(all_probs), dim=0)
            predictions = torch.argmax(avg_probs, dim=1).cpu().numpy()
            
            # æ”¶é›†ç»“æœ
            all_soft_predictions.extend(predictions)
            all_soft_probs.extend(avg_probs.cpu().numpy())
            all_test_labels.extend(labels)
            
            # è®¡ç®—å‡†ç¡®ç‡
            correct = (predictions == labels).sum()
            soft_voting_acc += correct
            total_samples += len(labels)
    
    ensemble_soft_acc = soft_voting_acc / total_samples
    ensemble_soft_f1 = f1_score(all_test_labels, all_soft_predictions, average='macro')
    print(f"é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰æµ‹è¯•å‡†ç¡®ç‡: {ensemble_soft_acc*100:.2f}%")
    print(f"é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰æµ‹è¯•F1-score: {ensemble_soft_f1*100:.2f}%")
    
    # è¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰
    print("\nè¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰...")
    hard_voting_acc = 0
    total_samples = 0
    all_hard_predictions = []
    
    with torch.no_grad():
        for batch in tqdm(test_loader, desc="é›†æˆè¯„ä¼°ï¼ˆç¡¬æŠ•ç¥¨ï¼‰"):
            input_ids = batch['input_ids'].to(config.device)
            attention_mask = batch['attention_mask'].to(config.device)
            labels = batch['labels'].cpu().numpy()
            
            # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ - ä½¿ç”¨é¢„å…ˆåŠ è½½çš„æ¨¡å‹
            all_predictions = []
            for model in loaded_models:
                outputs = model(input_ids=input_ids, attention_mask=attention_mask)
                predictions = torch.argmax(outputs.logits, dim=1).cpu().numpy()
                all_predictions.append(predictions)
            
            # ç¡¬æŠ•ç¥¨ï¼šå¤šæ•°ç¥¨
            all_predictions = np.array(all_predictions)
            final_predictions = []
            
            for i in range(all_predictions.shape[1]):
                votes = all_predictions[:, i]
                vote_0 = np.sum(votes == 0)
                vote_1 = np.sum(votes == 1)
                final_predictions.append(0 if vote_0 > vote_1 else 1)
            
            final_predictions = np.array(final_predictions)
            
            # æ”¶é›†ç»“æœ
            all_hard_predictions.extend(final_predictions)
            
            # è®¡ç®—å‡†ç¡®ç‡
            correct = (final_predictions == labels).sum()
            hard_voting_acc += correct
            total_samples += len(labels)
    
    ensemble_hard_acc = hard_voting_acc / total_samples
    ensemble_hard_f1 = f1_score(all_test_labels, all_hard_predictions, average='macro')
    print(f"é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰æµ‹è¯•å‡†ç¡®ç‡: {ensemble_hard_acc*100:.2f}%")
    print(f"é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰æµ‹è¯•F1-score: {ensemble_hard_f1*100:.2f}%")
    
    # 6. ç”Ÿæˆé›†æˆæ¨¡å‹å¯è§†åŒ–
    print("\n" + "="*70)
    print("ğŸ“ˆ 6. ç”Ÿæˆé›†æˆæ¨¡å‹å¯è§†åŒ–")
    print("="*70)
    
    # è½¬æ¢åˆ—è¡¨ä¸ºnumpyæ•°ç»„
    all_test_labels = np.array(all_test_labels)
    all_soft_predictions = np.array(all_soft_predictions)
    all_soft_probs = np.array(all_soft_probs)
    all_hard_predictions = np.array(all_hard_predictions)
    
    # ç»˜åˆ¶é›†æˆæ¨¡å‹æ··æ·†çŸ©é˜µ
    plot_confusion_matrix_comprehensive(all_test_labels, all_soft_predictions, 
                                      model_name='ensemble_soft')
    plot_confusion_matrix_comprehensive(all_test_labels, all_hard_predictions,
                                      model_name='ensemble_hard')
    
    # 7. ä¿å­˜çœŸæ­£çš„é›†æˆæ¨¡å‹
    print("\n" + "="*70)
    print("ğŸ’¾ 7. ä¿å­˜çœŸæ­£çš„é›†æˆæ¨¡å‹")
    print("="*70)
    
    true_ensemble_data = {
        'models': all_model_states,
        'model_configs': all_model_configs,
        'tokenizer_info': tokenizer.name_or_path,
        
        'performance': {
            'single_model_acc': float(single_test_acc),
            'single_model_f1': float(single_test_f1),
            'ensemble_soft_acc': float(ensemble_soft_acc),
            'ensemble_soft_f1': float(ensemble_soft_f1),
            'ensemble_hard_acc': float(ensemble_hard_acc),
            'ensemble_hard_f1': float(ensemble_hard_f1),
            'improvement_soft_acc': float(ensemble_soft_acc - single_test_acc),
            'improvement_soft_f1': float(ensemble_soft_f1 - single_test_f1),
            'improvement_hard_acc': float(ensemble_hard_acc - single_test_acc),
            'improvement_hard_f1': float(ensemble_hard_f1 - single_test_f1),
        },
        
        'fold_results': all_fold_results,
        'model_class': 'BertForSequenceClassification',
        'tokenizer_class': 'BertTokenizer',
        'max_len': config.max_length,
        
        'version': '2.0-cross-validation',
        'created_date': time.strftime('%Y-%m-%d'),
        'device': str(config.device),
        'description': 'BERTæƒ…æ„Ÿåˆ†æ - 5æŠ˜äº¤å‰éªŒè¯é›†æˆæ¨¡å‹'
    }
    
    torch.save(true_ensemble_data, 'bert_true_ensemble_model_cv.pt')
    print(f"âœ“ çœŸæ­£çš„é›†æˆæ¨¡å‹å·²ä¿å­˜åˆ° bert_true_ensemble_model_cv.pt")
    
    # åŒæ—¶ä¿å­˜ä¸€ä¸ªtransformersæ ¼å¼çš„é›†æˆæ¨¡å‹ï¼ˆä½¿ç”¨ç¬¬ä¸€æŠ˜ä½œä¸ºä»£è¡¨ï¼‰
    single_model.save_pretrained('./bert_ensemble_representative')
    tokenizer.save_pretrained('./bert_ensemble_representative')
    
    # 8. æ€»ç»“
    print("\n" + "="*70)
    print("ğŸ‰ BERTæ¨¡å‹è®­ç»ƒå®Œæˆæ€»ç»“")
    print("="*70)
    
    print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½:")
    print(f"  å•æ¨¡å‹: å‡†ç¡®ç‡={single_test_acc*100:.2f}%, F1={single_test_f1*100:.2f}%")
    print(f"  é›†æˆè½¯æŠ•ç¥¨: å‡†ç¡®ç‡={ensemble_soft_acc*100:.2f}%, F1={ensemble_soft_f1*100:.2f}%")
    print(f"  é›†æˆç¡¬æŠ•ç¥¨: å‡†ç¡®ç‡={ensemble_hard_acc*100:.2f}%, F1={ensemble_hard_f1*100:.2f}%")
    
    soft_improvement = ensemble_soft_acc - single_test_acc
    hard_improvement = ensemble_hard_acc - single_test_acc
    
    print(f"\nğŸ“ˆ æ€§èƒ½æå‡:")
    print(f"  è½¯æŠ•ç¥¨æå‡: å‡†ç¡®ç‡ +{soft_improvement*100:.2f}%, F1 +{(ensemble_soft_f1-single_test_f1)*100:.2f}%")
    print(f"  ç¡¬æŠ•ç¥¨æå‡: å‡†ç¡®ç‡ +{hard_improvement*100:.2f}%, F1 +{(ensemble_hard_f1-single_test_f1)*100:.2f}%")
    
    print(f"\nğŸ’¾ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. å•æŠ˜æ¨¡å‹: bert_fold_*_best.pth (5ä¸ª)")
    print(f"  2. å•æŠ˜æ¨¡å‹(transformers): bert_fold_*_best_transformers/ (5ä¸ª)")
    print(f"  3. é›†æˆæ¨¡å‹: bert_true_ensemble_model_cv.pt")
    print(f"  4. ä»£è¡¨æ¨¡å‹: bert_ensemble_representative/")
    print(f"  5. å¯è§†åŒ–æ–‡ä»¶: {config.viz_dir}/ ç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡")
    
    return true_ensemble_data

# ==================== 9. é¢„æµ‹å‡½æ•° ====================
def predict_with_bert_ensemble(text, ensemble_path='bert_true_ensemble_model_cv.pt', device=None):
    """ä½¿ç”¨é›†æˆBERTæ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    import torch
    from transformers import BertTokenizer, BertForSequenceClassification
    
    # è®¾ç½®è®¾å¤‡
    if device is None:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    
    # åŠ è½½é›†æˆæ¨¡å‹
    ensemble_data = torch.load(ensemble_path, map_location='cpu', weights_only=False)
    
    # åŠ è½½tokenizer
    tokenizer = BertTokenizer.from_pretrained(ensemble_data['tokenizer_info'])
    
    # å¤„ç†è¾“å…¥æ–‡æœ¬
    encoding = tokenizer.encode_plus(
        text,
        truncation=True,
        padding='max_length',
        max_length=128,
        return_tensors='pt'
    )
    
    input_ids = encoding['input_ids'].to(device)
    attention_mask = encoding['attention_mask'].to(device)
    
    # é›†æˆé¢„æµ‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰
    all_probs = []
    for i, (state_dict, model_config) in enumerate(zip(ensemble_data['models'], 
                                                      ensemble_data['model_configs'])):
        model = BertForSequenceClassification.from_pretrained(
            model_config['model_path'],
            num_labels=model_config['num_labels']
        )
        model.load_state_dict(state_dict)
        model.to(device)  # ç§»åŠ¨åˆ°è®¾å¤‡
        model.eval()
        
        with torch.no_grad():
            outputs = model(input_ids=input_ids, attention_mask=attention_mask)
            probs = torch.softmax(outputs.logits, dim=1)
            all_probs.append(probs)
    
    # å¹³å‡æ¦‚ç‡
    avg_probs = torch.mean(torch.stack(all_probs), dim=0)
    prediction = torch.argmax(avg_probs, dim=1).item()
    confidence = torch.max(avg_probs).item()
    
    sentiment = "æ­£é¢" if prediction == 1 else "è´Ÿé¢"
    
    return {
        'text': text,
        'sentiment': sentiment,
        'confidence': confidence,
        'prediction': prediction,
        'probabilities': avg_probs.cpu().numpy().tolist()[0],  # ç§»å›CPU
        'device': str(device)
    }

# ==================== 10. è¿è¡Œ ====================
if __name__ == "__main__":
    try:
        # è¿è¡Œ5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ
        ensemble_data = main_bert_cross_validation()
        
        # ç¤ºä¾‹ï¼šä½¿ç”¨é›†æˆæ¨¡å‹è¿›è¡Œé¢„æµ‹
        print("\n" + "="*70)
        print("ğŸ¤– ç¤ºä¾‹é¢„æµ‹")
        print("="*70)
        
        test_texts = [
            "è¿™ä¸ªå•†å“è´¨é‡çœŸçš„å¾ˆå¥½ï¼Œéå¸¸æ»¡æ„ï¼",
            "ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†æ•´æ•´ä¸€ä¸ªæ˜ŸæœŸ",
            "æ€§ä»·æ¯”å¾ˆé«˜ï¼Œæ¨èè´­ä¹°",
            "åŒ…è£…ç ´æŸï¼Œå•†å“æœ‰ç‘•ç–µ"
        ]
        
        for text in test_texts:
            result = predict_with_bert_ensemble(text)
            print(f"\næ–‡æœ¬: {text}")
            print(f"æƒ…æ„Ÿ: {result['sentiment']} (ç½®ä¿¡åº¦: {result['confidence']:.2%})")
            
    except Exception as e:
        print(f"\nç¨‹åºé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()

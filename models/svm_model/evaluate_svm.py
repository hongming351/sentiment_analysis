# evaluate_svm.py
import warnings
warnings.filterwarnings('ignore')

import os
import pandas as pd
import numpy as np
import pickle
import time
import json
import matplotlib
from pathlib import Path
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
import seaborn as sns
matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'STXihei']
matplotlib.rcParams['axes.unicode_minus'] = False
print("=" * 70)
print("äº¬ä¸œè¯„è®ºæƒ…æ„Ÿåˆ†æ - SVMæ¨¡å‹è¯„ä¼°")
print("=" * 70)

# ==================== æ–‡æœ¬é¢„å¤„ç† ====================
def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    
    import re
    text = str(text).strip()
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?;\'"ã€]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def tokenize_chinese(text, use_jieba=True):
    """ä¸­æ–‡åˆ†è¯"""
    import jieba
    text = clean_text(text)
    if not text:
        return ""
    
    if use_jieba:
        tokens = jieba.lcut(text)
    else:
        tokens = list(text)
    
    tokens = [token.strip() for token in tokens if token.strip()]
    return ' '.join(tokens)

# ==================== åŠ è½½æ•°æ® ====================
def load_test_data(data_dir=None):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    import os
    import pandas as pd
    
    if data_dir is None:
        data_dir = r"D:\jd_changed12.11\data"
    
    test_path = os.path.join(data_dir, "dev.csv")
    print(f"æµ‹è¯•æ–‡ä»¶è·¯å¾„: {test_path}")
    
    if not os.path.exists(test_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æµ‹è¯•é›†æ–‡ä»¶: {test_path}")
    
    # åŠ è½½æ•°æ®
    test_df = pd.read_csv(test_path)
    print(f"åŸå§‹æµ‹è¯•é›†: {len(test_df)} æ¡")
    
    # æ£€æŸ¥å¹¶æ¸…ç†NaN
    original_len = len(test_df)
    test_df_clean = test_df.dropna(subset=['sentence', 'label'])
    
    print(f"æ¸…ç†åæµ‹è¯•é›†: {len(test_df_clean)} æ¡")
    print(f"ç§»é™¤äº† {original_len - len(test_df_clean)} æ¡åŒ…å«NaNçš„æ•°æ®")
    
    if len(test_df_clean) == 0:
        raise ValueError("æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print("\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
    label_counts = test_df_clean['label'].value_counts().sort_index()
    for label, count in label_counts.items():
        percentage = count / len(test_df_clean) * 100
        print(f"  æ ‡ç­¾ {label}: {count} æ¡ ({percentage:.1f}%)")
    
    return test_df_clean
# ==================== åŠ è½½æ¨¡å‹ ====================
def load_svm_models(n_folds=5):
    """åŠ è½½æ‰€æœ‰SVMæ¨¡å‹ - æ™ºèƒ½è·¯å¾„æ£€æµ‹"""
    import os
    
    # å¯èƒ½çš„æ¨¡å‹è·¯å¾„åˆ—è¡¨ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åºï¼‰
    possible_dirs = [
        # 1. ç¡¬ç¼–ç ç»å¯¹è·¯å¾„
        r"D:\jd_changed12.11\models\svm_model\svm_models",
        
        # 2. ç›¸å¯¹äºå½“å‰è„šæœ¬çš„è·¯å¾„
        os.path.join(os.path.dirname(os.path.abspath(__file__)), "svm_models"),
        
        # 3. ç›¸å¯¹äºå½“å‰å·¥ä½œç›®å½•çš„è·¯å¾„
        os.path.join(os.getcwd(), "svm_models"),
        os.path.join(os.getcwd(), "models", "svm_model", "svm_models"),
        
        # 4. å…¶ä»–å¯èƒ½çš„è·¯å¾„
        "svm_models",
        "../svm_models",
        "../../svm_models",
    ]
    
    # æŸ¥æ‰¾å­˜åœ¨çš„ç›®å½•
    model_dir = None
    for dir_path in possible_dirs:
        print(f"æ£€æŸ¥ç›®å½•: {dir_path}")
        if os.path.exists(dir_path):
            # æ£€æŸ¥ç›®å½•ä¸­æ˜¯å¦æœ‰æ¨¡å‹æ–‡ä»¶
            has_models = any(f"svm_fold_{i}.pkl" in os.listdir(dir_path) 
                           for i in range(n_folds) if os.path.exists(dir_path))
            if has_models:
                model_dir = dir_path
                print(f"âœ“ æ‰¾åˆ°æ¨¡å‹ç›®å½•: {model_dir}")
                break
    
    if model_dir is None:
        raise FileNotFoundError("æ‰¾ä¸åˆ°åŒ…å«SVMæ¨¡å‹çš„ç›®å½•")
    
    print(f"ä½¿ç”¨æ¨¡å‹ç›®å½•: {model_dir}")
    print(f"ç›®å½•å†…å®¹: {os.listdir(model_dir)}")
    
    models = []
    
    for fold_idx in range(n_folds):
        model_path = os.path.join(model_dir, f"svm_fold_{fold_idx}.pkl")
        
        if not os.path.exists(model_path):
            print(f"âš ï¸ è­¦å‘Š: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
            # è·³è¿‡è¿™ä¸ªæ¨¡å‹ï¼Œç»§ç»­åŠ è½½å…¶ä»–çš„
            continue
        
        try:
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
                models.append(model)
            print(f"âœ“ åŠ è½½ç¬¬{fold_idx+1}æŠ˜æ¨¡å‹æˆåŠŸ")
        except Exception as e:
            print(f"âŒ åŠ è½½ç¬¬{fold_idx+1}æŠ˜æ¨¡å‹å¤±è´¥: {e}")
    
    if len(models) == 0:
        raise FileNotFoundError("æœªèƒ½åŠ è½½ä»»ä½•æ¨¡å‹")
    
    print(f"æˆåŠŸåŠ è½½äº† {len(models)} ä¸ªæ¨¡å‹")
    return models
# ==================== é›†æˆé¢„æµ‹ ====================
def ensemble_predict(models, texts, voting='soft'):
    """é›†æˆé¢„æµ‹"""
    if voting == 'hard':
        # ç¡¬æŠ•ç¥¨
        all_predictions = []
        for model in models:
            pred = model.predict(texts)
            all_predictions.append(pred)
        
        all_predictions = np.array(all_predictions)
        final_predictions = []
        
        for i in range(all_predictions.shape[1]):
            votes = all_predictions[:, i]
            vote_counts = np.bincount(votes)
            final_predictions.append(np.argmax(vote_counts))
        
        return np.array(final_predictions)
    
    else:
        # è½¯æŠ•ç¥¨
        try:
            all_probs = []
            for model in models:
                probs = model.decision_function(texts)
                # å°†å†³ç­–å‡½æ•°å€¼è½¬æ¢ä¸ºæ¦‚ç‡
                probs = 1 / (1 + np.exp(-probs))
                probs = np.column_stack([1-probs, probs])  # è½¬æ¢ä¸ºäºŒåˆ†ç±»æ¦‚ç‡
                all_probs.append(probs)
            
            avg_probs = np.mean(all_probs, axis=0)
            return np.argmax(avg_probs, axis=1)
        except:
            print("âš ï¸  æ— æ³•è¿›è¡Œè½¯æŠ•ç¥¨ï¼Œä½¿ç”¨ç¡¬æŠ•ç¥¨")
            return ensemble_predict(models, texts, voting='hard')

# ==================== å¯è§†åŒ–å‡½æ•° ====================
def plot_confusion_matrix(cm, title='æ··æ·†çŸ©é˜µ', save_path=None):
    """ç»˜åˆ¶æ··æ·†çŸ©é˜µ"""
    plt.figure(figsize=(8, 6))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['è´Ÿé¢', 'æ­£é¢'],
                yticklabels=['è´Ÿé¢', 'æ­£é¢'])
    plt.title(title, fontsize=14, fontweight='bold')
    plt.xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    plt.ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {save_path}")
    
    plt.show()

def plot_performance_comparison(single_acc, ensemble_soft_acc, ensemble_hard_acc, save_path=None):
    """ç»˜åˆ¶æ€§èƒ½å¯¹æ¯”å›¾"""
    models = ['å•æ¨¡å‹', 'é›†æˆæ¨¡å‹\n(è½¯æŠ•ç¥¨)', 'é›†æˆæ¨¡å‹\n(ç¡¬æŠ•ç¥¨)']
    accuracies = [single_acc * 100, ensemble_soft_acc * 100, ensemble_hard_acc * 100]
    improvements = [0, (ensemble_soft_acc - single_acc) * 100, (ensemble_hard_acc - single_acc) * 100]
    
    colors = ['lightblue', 'lightgreen', 'lightcoral']
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, acc, imp) in enumerate(zip(bars, accuracies, improvements)):
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.3,
               f'{acc:.2f}%', ha='center', va='bottom', fontsize=11)
        
        if i > 0:
            ax.text(bar.get_x() + bar.get_width()/2., height/2,
                   f'+{imp:.2f}%', ha='center', va='center',
                   fontsize=10, fontweight='bold', color='red')
    
    ax.set_ylabel('æµ‹è¯•å‡†ç¡®ç‡ (%)', fontsize=12)
    ax.set_title('SVMæ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=14, fontweight='bold')
    ax.set_ylim([min(accuracies)-2, max(accuracies)+2])
    ax.grid(True, alpha=0.3, axis='y')
    
    plt.tight_layout()
    
    if save_path:
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        print(f"âœ“ æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜: {save_path}")
    
    plt.show()

# ==================== ä¸»å‡½æ•° ====================
def main():
    # é…ç½®
    config = {
        'data_dir': r"D:\jd_changed12.11\data",
        'model_dir': r"D:\jd_changed12.11\models\svm_model\svm_models",
        'text_column': 'sentence',
        'label_column': 'label',
        'n_folds': 5,
        'seed': 42,
        'results_dir': 'evaluation_results' 
    }
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(config['results_dir'], exist_ok=True)
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("\n" + "="*70)
    print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®")
    print("="*70)
    
    try:
        test_df = load_test_data(config['data_dir'])
        print(f"âœ“ æµ‹è¯•é›†: {len(test_df)} æ¡è¯„è®º")
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        return
    
    # æ–‡æœ¬é¢„å¤„ç†
    print("æ–‡æœ¬é¢„å¤„ç†ä¸­...")
    test_texts = test_df[config['text_column']].apply(tokenize_chinese).tolist()
    test_labels = test_df[config['label_column']].tolist()
    
    print(f"âœ“ é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬: {len(test_texts)}")
    
    # åŠ è½½æ¨¡å‹
    print("\n" + "="*70)
    print("ğŸ¤– åŠ è½½SVMæ¨¡å‹")
    print("="*70)
    
    try:
        svm_models = load_svm_models(n_folds=config['n_folds'])
        print(f"âœ“ æˆåŠŸåŠ è½½ {len(svm_models)} ä¸ªæ¨¡å‹")
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print(f"è¯·å…ˆè¿è¡Œ train_svm.py è®­ç»ƒæ¨¡å‹")
        return
    
    # è¯„ä¼°å•æ¨¡å‹ï¼ˆç¬¬ä¸€æŠ˜ï¼‰
    print("\n" + "="*70)
    print("ğŸ“ˆ è¯„ä¼°å•æ¨¡å‹ï¼ˆç¬¬ä¸€æŠ˜ï¼‰")
    print("="*70)
    
    single_model = svm_models[0]
    single_predictions = single_model.predict(test_texts)
    single_acc = accuracy_score(test_labels, single_predictions)
    single_f1 = f1_score(test_labels, single_predictions, average='weighted')
    
    print(f"å•æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {single_acc*100:.2f}%")
    print(f"å•æ¨¡å‹æµ‹è¯•F1-score: {single_f1*100:.2f}%")
    
    # è®¡ç®—å•æ¨¡å‹æ··æ·†çŸ©é˜µ
    single_cm = confusion_matrix(test_labels, single_predictions)
    
    # è¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰
    print("\n" + "="*70)
    print("ğŸ¤ è¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰")
    print("="*70)
    
    ensemble_soft_predictions = ensemble_predict(svm_models, test_texts, voting='soft')
    ensemble_soft_acc = accuracy_score(test_labels, ensemble_soft_predictions)
    ensemble_soft_f1 = f1_score(test_labels, ensemble_soft_predictions, average='weighted')
    
    print(f"é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰æµ‹è¯•å‡†ç¡®ç‡: {ensemble_soft_acc*100:.2f}%")
    print(f"é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰æµ‹è¯•F1-score: {ensemble_soft_f1*100:.2f}%")
    print(f"ç›¸è¾ƒäºå•æ¨¡å‹æå‡: +{(ensemble_soft_acc - single_acc)*100:.2f}%")
    
    # è®¡ç®—é›†æˆæ¨¡å‹æ··æ·†çŸ©é˜µ
    ensemble_soft_cm = confusion_matrix(test_labels, ensemble_soft_predictions)
    
    # è¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰
    print("\n" + "="*70)
    print("ğŸ¤ è¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰")
    print("="*70)
    
    ensemble_hard_predictions = ensemble_predict(svm_models, test_texts, voting='hard')
    ensemble_hard_acc = accuracy_score(test_labels, ensemble_hard_predictions)
    ensemble_hard_f1 = f1_score(test_labels, ensemble_hard_predictions, average='weighted')
    
    print(f"é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰æµ‹è¯•å‡†ç¡®ç‡: {ensemble_hard_acc*100:.2f}%")
    print(f"é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰æµ‹è¯•F1-score: {ensemble_hard_f1*100:.2f}%")
    print(f"ç›¸è¾ƒäºå•æ¨¡å‹æå‡: +{(ensemble_hard_acc - single_acc)*100:.2f}%")
    
    # è®¡ç®—é›†æˆæ¨¡å‹æ··æ·†çŸ©é˜µ
    ensemble_hard_cm = confusion_matrix(test_labels, ensemble_hard_predictions)
    
    # ç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
    print("\n" + "="*70)
    print("ğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Šï¼ˆé›†æˆæ¨¡å‹-è½¯æŠ•ç¥¨ï¼‰")
    print("="*70)
    
    print(classification_report(test_labels, ensemble_soft_predictions,
                               target_names=['è´Ÿé¢', 'æ­£é¢'],
                               digits=4))
    
    # ==================== å¯è§†åŒ– ====================
    print("\n" + "="*70)
    print("ğŸ“Š ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨")
    print("="*70)
    
    # 1. æ··æ·†çŸ©é˜µ
    plot_confusion_matrix(single_cm, 
                         title='SVMå•æ¨¡å‹æ··æ·†çŸ©é˜µ',
                         save_path=os.path.join(config['results_dir'], 'svm_single_cm.png'))
    
    plot_confusion_matrix(ensemble_soft_cm,
                         title='SVMé›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰æ··æ·†çŸ©é˜µ',
                         save_path=os.path.join(config['results_dir'], 'svm_ensemble_soft_cm.png'))
    
    plot_confusion_matrix(ensemble_hard_cm,
                         title='SVMé›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰æ··æ·†çŸ©é˜µ',
                         save_path=os.path.join(config['results_dir'], 'svm_ensemble_hard_cm.png'))
    
    # 2. æ€§èƒ½å¯¹æ¯”å›¾
    plot_performance_comparison(single_acc, ensemble_soft_acc, ensemble_hard_acc,
                               save_path=os.path.join(config['results_dir'], 'svm_performance_comparison.png'))
    
    # ==================== ä¿å­˜ç»“æœ ====================
    print("\n" + "="*70)
    print("ğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ")
    print("="*70)
    
    evaluation_results = {
        'single_model': {
            'accuracy': float(single_acc),
            'f1_score': float(single_f1),
            'confusion_matrix': single_cm.tolist()
        },
        'ensemble_soft': {
            'accuracy': float(ensemble_soft_acc),
            'f1_score': float(ensemble_soft_f1),
            'improvement': float(ensemble_soft_acc - single_acc),
            'confusion_matrix': ensemble_soft_cm.tolist()
        },
        'ensemble_hard': {
            'accuracy': float(ensemble_hard_acc),
            'f1_score': float(ensemble_hard_f1),
            'improvement': float(ensemble_hard_acc - single_acc),
            'confusion_matrix': ensemble_hard_cm.tolist()
        },
        'test_set_size': len(test_texts),
        'evaluation_date': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    results_file = os.path.join(config['results_dir'], 'svm_evaluation_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation_results, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ°: {results_file}")
    
    # ==================== æ€»ç»“ ====================
    print("\n" + "="*70)
    print("âœ… SVMæ¨¡å‹è¯„ä¼°å®Œæˆ")
    print("="*70)
    
    print(f"\nğŸ“Š æœ€ç»ˆç»“æœ:")
    print(f"  å•æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {single_acc*100:.2f}%")
    print(f"  é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰: {ensemble_soft_acc*100:.2f}% (+{(ensemble_soft_acc - single_acc)*100:.2f}%)")
    print(f"  é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰: {ensemble_hard_acc*100:.2f}% (+{(ensemble_hard_acc - single_acc)*100:.2f}%)")
    
    print(f"\nğŸ’¾ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. svm_single_cm.png - å•æ¨¡å‹æ··æ·†çŸ©é˜µ")
    print(f"  2. svm_ensemble_soft_cm.png - é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰æ··æ·†çŸ©é˜µ")
    print(f"  3. svm_ensemble_hard_cm.png - é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰æ··æ·†çŸ©é˜µ")
    print(f"  4. svm_performance_comparison.png - æ€§èƒ½å¯¹æ¯”å›¾")
    print(f"  5. svm_evaluation_results.json - è¯¦ç»†è¯„ä¼°ç»“æœ")


if __name__ == "__main__":
    main()
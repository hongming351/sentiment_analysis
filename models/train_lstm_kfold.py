# train_jd_lstm_cpu.py
import warnings
warnings.filterwarnings('ignore')
import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
import re
import jieba
import matplotlib.pyplot as plt
from tqdm import tqdm
from collections import Counter, defaultdict
import collections
import glob
from torch.utils.data import Dataset
print("=" * 70)
print("äº¬ä¸œè¯„è®ºæƒ…æ„Ÿåˆ†æ - LSTMæ¨¡å‹è®­ç»ƒ (ä½¿ç”¨äº¤å‰éªŒè¯ - CPUè®­ç»ƒ)")
print("=" * 70)
# ==================== 1. è®¾ç½®éšæœºç§å­ ====================
def set_seed(seed=1234):
    np.random.seed(seed)
    torch.manual_seed(seed)
    # CPUè®­ç»ƒï¼Œä¸è®¾ç½®CUDAç›¸å…³ç§å­
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(1234)

# ==================== 2. æ£€æŸ¥å¹¶å®‰è£…jieba ====================
try:
    import jieba
    print("âœ“ jieba å·²å®‰è£…")
except ImportError:
    print("æ­£åœ¨å®‰è£… jieba...")
    import subprocess
    subprocess.check_call([sys.executable, "-m", "pip", "install", "jieba"])
    import jieba
    print("âœ“ jieba å®‰è£…å®Œæˆ")

# ==================== 3. æ–‡æœ¬é¢„å¤„ç†å‡½æ•° ====================
def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    
    text = str(text).strip()
    # ç§»é™¤HTMLæ ‡ç­¾
    text = re.sub(r'<[^>]+>', '', text)
    
    # ç§»é™¤URL
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)

    # ç§»é™¤é‚®ç®±
    text = re.sub(r'\S+@\S+', '', text)
    # ä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?;\'"ã€]', ' ', text)
    # åˆå¹¶å¤šä¸ªç©ºæ ¼
    text = re.sub(r'\s+', ' ', text)
    return text.strip()
def tokenize_chinese(text, use_jieba=True):
    """ä¸­æ–‡åˆ†è¯"""
    text = clean_text(text)
    if not text:
        return []
    
    if use_jieba:
        # ä½¿ç”¨jiebaåˆ†è¯
        tokens = jieba.lcut(text)
    else:
        # ç®€å•æŒ‰å­—ç¬¦åˆ†å‰²ï¼ˆå¤‡ç”¨æ–¹æ¡ˆï¼‰
        tokens = list(text)
    
    # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²
    tokens = [token.strip() for token in tokens if token.strip()]
    
    return tokens

# ==================== 4. è‡ªå®šä¹‰è¯æ±‡è¡¨ç±» (ç®€åŒ–ç‰ˆ) ====================
class Vocabulary:
    def __init__(self, min_freq=2):
        self.min_freq = min_freq
        self.word2idx = {}
        self.idx2word = {}
        self.word_freq = Counter()
        
        # ç‰¹æ®Štokenï¼ˆå‚è€ƒIMDBä»£ç ï¼Œåªæœ‰unkå’Œpadï¼‰
        self.unk_token = '<unk>'
        self.pad_token = '<pad>'
        
        # æ·»åŠ ç‰¹æ®Štoken
        self.add_word(self.unk_token)
        self.add_word(self.pad_token)
    
    def add_word(self, word):
        if word not in self.word2idx:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word
    
    def build_vocab(self, texts, tokenizer_fn):
        """ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨"""
        print("æ„å»ºè¯æ±‡è¡¨ä¸­...")
        
        # ç»Ÿè®¡æ‰€æœ‰è¯é¢‘
        word_freq = Counter()
        for text in tqdm(texts, desc="å¤„ç†æ–‡æœ¬"):
            tokens = tokenizer_fn(text)
            word_freq.update(tokens)
        
        # è¿‡æ»¤ä½é¢‘è¯
        for word, freq in word_freq.items():
            if freq >= self.min_freq:
                self.add_word(word)
        
        print(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå¤§å°: {len(self)}")
        print(f"  ç‰¹æ®Štoken: 2ä¸ª (<unk>, <pad>)")
        print(f"  æ™®é€šè¯æ±‡: {len(self) - 2} ä¸ª")
        print(f"  æœ€ä½è¯é¢‘: {self.min_freq}")
    
    def __len__(self):
        return len(self.word2idx)
    
    def word_to_index(self, word):
        """è·å–è¯çš„ç´¢å¼•ï¼Œä¸å­˜åœ¨åˆ™è¿”å›UNKç´¢å¼•"""
        return self.word2idx.get(word, self.word2idx[self.unk_token])
    
    def index_to_word(self, idx):
        """è·å–ç´¢å¼•å¯¹åº”çš„è¯"""
        return self.idx2word.get(idx, self.unk_token)
    
    def lookup_indices(self, tokens):
        """å°†tokenåˆ—è¡¨è½¬æ¢ä¸ºç´¢å¼•åˆ—è¡¨ï¼ˆå‚è€ƒIMDBä»£ç ä¸­çš„vocab.lookup_indicesï¼‰"""
        indices = []
        for token in tokens:
            indices.append(self.word_to_index(token))
        return indices
    
    def set_default_index(self, idx):
        """è®¾ç½®é»˜è®¤ç´¢å¼•ï¼ˆä¸ºäº†å…¼å®¹IMDBä»£ç æ ¼å¼ï¼‰"""
        # åœ¨è¿™ä¸ªç®€åŒ–å®ç°ä¸­ï¼Œæˆ‘ä»¬å·²ç»åœ¨word_to_indexä¸­å¤„ç†äº†UNK
        pass

# ==================== 5. æ•°æ®é›†ç±» ====================
class JDDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_length=128, tokenizer_fn=None):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_length = max_length
        
        if tokenizer_fn is None:
            self.tokenizer_fn = tokenize_chinese
        else:
            self.tokenizer_fn = tokenizer_fn
        
        # é¢„å¤„ç†æ‰€æœ‰æ–‡æœ¬
        print("é¢„å¤„ç†æ•°æ®ä¸­...")
        self.tokens_list = []
        self.lengths = []
        self.ids_list = []
        self.filtered_texts = []
        self.filtered_labels = []
        
        for text, label in tqdm(zip(texts, labels), desc="å¤„ç†æ–‡æœ¬", total=len(texts)):
            tokens = self.tokenizer_fn(text)[:max_length]
            length = len(tokens)
            
            # è·³è¿‡é•¿åº¦ä¸º0çš„æ ·æœ¬
            if length == 0:
                continue
                
            ids = self.vocab.lookup_indices(tokens)
            
            self.tokens_list.append(tokens)
            self.lengths.append(length)
            self.ids_list.append(ids)
            self.filtered_texts.append(text)
            self.filtered_labels.append(label)
        
        # æ›´æ–°è¿‡æ»¤åçš„æ•°æ®
        self.texts = self.filtered_texts
        self.labels = self.filtered_labels
        
        print(f"  åŸå§‹æ ·æœ¬æ•°: {len(texts)}")
        print(f"  è¿‡æ»¤åæ ·æœ¬æ•°: {len(self.texts)} (ç§»é™¤äº†{len(texts)-len(self.texts)}ä¸ªç©ºæ–‡æœ¬)")
    
    def __len__(self):
        return len(self.texts)
    
    def __getitem__(self, idx):
        return {
            'ids': torch.tensor(self.ids_list[idx], dtype=torch.long),
            'length': torch.tensor(self.lengths[idx], dtype=torch.long),
            'label': torch.tensor(self.labels[idx], dtype=torch.long),
            'tokens': self.tokens_list[idx]
        }

# ==================== 6. LSTMæ¨¡å‹ (å‚è€ƒIMDBä»£ç ç»“æ„) ====================
class LSTM(nn.Module):
    def __init__(
        self,
        vocab_size,
        embedding_dim,
        hidden_dim,
        output_dim,
        n_layers,
        bidirectional,
        dropout_rate,
        pad_index,
    ):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_index)
        self.lstm = nn.LSTM(
            embedding_dim,
            hidden_dim,
            n_layers,
            bidirectional=bidirectional,
            dropout=dropout_rate,
            batch_first=True,
        )
        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)
        self.dropout = nn.Dropout(dropout_rate)

    def forward(self, ids, length):
        # ids = [batch size, seq len]
        # length = [batch size]
        embedded = self.dropout(self.embedding(ids))
        # embedded = [batch size, seq len, embedding dim]
        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            embedded, length.cpu(), batch_first=True, enforce_sorted=False
        )
        packed_output, (hidden, cell) = self.lstm(packed_embedded)
        # hidden = [n layers * n directions, batch size, hidden dim]
        # cell = [n layers * n directions, batch size, hidden dim]
        output, output_length = nn.utils.rnn.pad_packed_sequence(packed_output)
        # output = [batch size, seq len, hidden dim * n directions]
        if self.lstm.bidirectional:
            hidden = self.dropout(torch.cat([hidden[-1], hidden[-2]], dim=-1))
            # hidden = [batch size, hidden dim * 2]
        else:
            hidden = self.dropout(hidden[-1])
            # hidden = [batch size, hidden dim]
        prediction = self.fc(hidden)
        # prediction = [batch size, output dim]
        return prediction

# ==================== 7. æ•°æ®åŠ è½½å™¨å‡½æ•° ====================
def get_collate_fn(pad_index):
    def collate_fn(batch):
        batch_ids = [i["ids"] for i in batch]
        batch_ids = nn.utils.rnn.pad_sequence(
            batch_ids, padding_value=pad_index, batch_first=True
        )
        batch_length = [i["length"] for i in batch]
        batch_length = torch.stack(batch_length)
        batch_label = [i["label"] for i in batch]
        batch_label = torch.stack(batch_label)
        batch = {"ids": batch_ids, "length": batch_length, "label": batch_label}
        return batch

    return collate_fn

def get_data_loader(dataset, batch_size, pad_index, shuffle=False):
    collate_fn = get_collate_fn(pad_index)
    data_loader = torch.utils.data.DataLoader(
        dataset=dataset,
        batch_size=batch_size,
        collate_fn=collate_fn,
        shuffle=shuffle,
    )
    return data_loader

# ==================== 8. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•° (å‚è€ƒIMDBä»£ç ) ====================
def train(dataloader, model, criterion, optimizer, device):
    model.train()
    epoch_losses = []
    epoch_accs = []
    for batch in tqdm(dataloader, desc="training..."):
        ids = batch["ids"].to(device)
        length = batch["length"].to(device)
        label = batch["label"].to(device)
        prediction = model(ids, length)
        loss = criterion(prediction, label)
        accuracy = get_accuracy(prediction, label)
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        epoch_losses.append(loss.item())
        epoch_accs.append(accuracy.item())
    return np.mean(epoch_losses), np.mean(epoch_accs)

def evaluate(dataloader, model, criterion, device):
    model.eval()
    epoch_losses = []
    epoch_accs = []
    with torch.no_grad():
        for batch in tqdm(dataloader, desc="evaluating..."):
            ids = batch["ids"].to(device)
            length = batch["length"].to(device)
            label = batch["label"].to(device)
            prediction = model(ids, length)
            loss = criterion(prediction, label)
            accuracy = get_accuracy(prediction, label)
            epoch_losses.append(loss.item())
            epoch_accs.append(accuracy.item())
    return np.mean(epoch_losses), np.mean(epoch_accs)

def get_accuracy(prediction, label):
    batch_size, _ = prediction.shape
    predicted_classes = prediction.argmax(dim=-1)
    correct_predictions = predicted_classes.eq(label).sum()
    accuracy = correct_predictions / batch_size
    return accuracy

def count_parameters(model):
    return sum(p.numel() for p in model.parameters() if p.requires_grad)

def initialize_weights(m):
    if isinstance(m, nn.Linear):
        nn.init.xavier_normal_(m.weight)
        nn.init.zeros_(m.bias)
    elif isinstance(m, nn.LSTM):
        for name, param in m.named_parameters():
            if "bias" in name:
                nn.init.zeros_(param)
            elif "weight" in name:
                nn.init.orthogonal_(param)

# ==================== 9. é¢„æµ‹å‡½æ•° ====================
def predict_sentiment(text, model, tokenizer_fn, vocab, device, max_length=256):
    tokens = tokenizer_fn(text)[:max_length]
    ids = vocab.lookup_indices(tokens)
    length = torch.LongTensor([len(ids)])
    tensor = torch.LongTensor(ids).unsqueeze(dim=0).to(device)
    prediction = model(tensor, length).squeeze(dim=0)
    probability = torch.softmax(prediction, dim=-1)
    predicted_class = prediction.argmax(dim=-1).item()
    predicted_probability = probability[predicted_class].item()
    return predicted_class, predicted_probability, probability[0].item(), probability[1].item()

# ==================== 10. æ•°æ®åŠ è½½å‡½æ•° ====================
def load_cross_validation_data(data_dir):
    """åŠ è½½äº¤å‰éªŒè¯æ•°æ®é›†"""
    print(f"æ­£åœ¨åŠ è½½äº¤å‰éªŒè¯æ•°æ®ï¼Œç›®å½•: {data_dir}")

    # æŸ¥æ‰¾æ‰€æœ‰è®­ç»ƒé›†å’ŒéªŒè¯é›†æ–‡ä»¶
    train_files = sorted(glob.glob(os.path.join(data_dir, "train_fold_*.csv")))
    valid_files = sorted(glob.glob(os.path.join(data_dir, "val_fold_*.csv")))

    if train_files and valid_files:
        # å¦‚æœæ‰¾åˆ°äº¤å‰éªŒè¯æ–‡ä»¶ï¼Œä½¿ç”¨å®ƒä»¬
        print(f"æ‰¾åˆ° {len(train_files)} ä¸ªè®­ç»ƒé›†æ–‡ä»¶å’Œ {len(valid_files)} ä¸ªéªŒè¯é›†æ–‡ä»¶")

        # åˆå¹¶æ‰€æœ‰è®­ç»ƒé›†å’ŒéªŒè¯é›†æ•°æ®
        all_train_dfs = []
        all_valid_dfs = []

        for train_file in train_files:
            df = pd.read_csv(train_file)
            # å¤„ç†ç¼ºå¤±å€¼
            df = df.dropna(subset=['sentence', 'label'])
            df['label'] = df['label'].fillna(0).astype(int)
            all_train_dfs.append(df)
            print(f"  åŠ è½½: {os.path.basename(train_file)} - {len(df)} æ¡æ•°æ®")

        for valid_file in valid_files:
            df = pd.read_csv(valid_file)
            # å¤„ç†ç¼ºå¤±å€¼
            df = df.dropna(subset=['sentence', 'label'])
            df['label'] = df['label'].fillna(0).astype(int)
            all_valid_dfs.append(df)
            print(f"  åŠ è½½: {os.path.basename(valid_file)} - {len(df)} æ¡æ•°æ®")

        # åˆå¹¶æ•°æ®
        train_df = pd.concat(all_train_dfs, ignore_index=True)
        valid_df = pd.concat(all_valid_dfs, ignore_index=True)

        print(f"âœ“ åˆå¹¶åè®­ç»ƒé›†: {len(train_df)} æ¡è¯„è®º")
        print(f"âœ“ åˆå¹¶åéªŒè¯é›†: {len(valid_df)} æ¡è¯„è®º")
    else:
        # å¦‚æœæ²¡æœ‰æ‰¾åˆ°äº¤å‰éªŒè¯æ–‡ä»¶ï¼Œä½¿ç”¨train.csvå’Œdev.csvä½œä¸ºæ›¿ä»£
        print("âš ï¸  æœªæ‰¾åˆ°äº¤å‰éªŒè¯æ–‡ä»¶ï¼Œä½¿ç”¨train.csvå’Œdev.csvä½œä¸ºæ›¿ä»£")

        # åŠ è½½train.csvä½œä¸ºè®­ç»ƒé›†
        train_file = os.path.join(data_dir, "train.csv")
        if not os.path.exists(train_file):
            raise FileNotFoundError(f"åœ¨ç›®å½• {data_dir} ä¸­æ‰¾ä¸åˆ°train.csvæ–‡ä»¶")

        train_df = pd.read_csv(train_file)
        # å¤„ç†ç¼ºå¤±å€¼
        train_df = train_df.dropna(subset=['sentence', 'label'])
        train_df['label'] = train_df['label'].fillna(0).astype(int)
        print(f"  åŠ è½½è®­ç»ƒé›†: train.csv - {len(train_df)} æ¡æ•°æ®")

        # åŠ è½½dev.csvä½œä¸ºéªŒè¯é›†
        valid_file = os.path.join(data_dir, "dev.csv")
        if not os.path.exists(valid_file):
            raise FileNotFoundError(f"åœ¨ç›®å½• {data_dir} ä¸­æ‰¾ä¸åˆ°dev.csvæ–‡ä»¶")

        valid_df = pd.read_csv(valid_file)
        # å¤„ç†ç¼ºå¤±å€¼
        valid_df = valid_df.dropna(subset=['sentence', 'label'])
        valid_df['label'] = valid_df['label'].fillna(0).astype(int)
        print(f"  åŠ è½½éªŒè¯é›†: dev.csv - {len(valid_df)} æ¡æ•°æ®")

        print(f"âœ“ ä½¿ç”¨train.csvä½œä¸ºè®­ç»ƒé›†: {len(train_df)} æ¡è¯„è®º")
        print(f"âœ“ ä½¿ç”¨dev.csvä½œä¸ºéªŒè¯é›†: {len(valid_df)} æ¡è¯„è®º")

    return train_df, valid_df

def load_test_data(test_file, data_dir=None):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    print(f"æ­£åœ¨åŠ è½½æµ‹è¯•æ•°æ®: {test_file}")
    
    # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•åœ¨data_dirä¸­æŸ¥æ‰¾
    if not os.path.exists(test_file) and data_dir:
        test_file_in_data_dir = os.path.join(data_dir, os.path.basename(test_file))
        if os.path.exists(test_file_in_data_dir):
            test_file = test_file_in_data_dir
            print(f"  åœ¨æ•°æ®ç›®å½•ä¸­æ‰¾åˆ°: {test_file}")
        else:
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°æµ‹è¯•æ–‡ä»¶: {test_file}")
    
    test_df = pd.read_csv(test_file)
    
    # å¤„ç†ç¼ºå¤±å€¼
    test_df = test_df.dropna(subset=['sentence', 'label'])  # åˆ é™¤sentenceæˆ–labelä¸ºNaNçš„è¡Œ
    test_df['label'] = test_df['label'].fillna(0).astype(int)  # å¡«å……å‰©ä½™çš„NaNä¸º0
    
    print(f"âœ“ æµ‹è¯•é›†: {len(test_df)} æ¡è¯„è®º (æ¸…ç†å)")
    
    return test_df

# ==================== 11. ä¸»å‡½æ•° ====================
def main():
    # ==================== è®¾å¤‡é€‰æ‹© ====================
    print("\n" + "="*70)
    print("ğŸ”§ è®¾å¤‡é€‰æ‹©ä¸é…ç½®")
    print("="*70)

    # ä¼˜å…ˆä½¿ç”¨GPUï¼Œå¦‚æœä¸å¯ç”¨åˆ™ä½¿ç”¨CPU
    if torch.cuda.is_available():
        device = torch.device("cuda")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"  âœ… GPUå¯ç”¨ - ä½¿ç”¨GPUåŠ é€Ÿè®­ç»ƒ")
        print(f"  GPUå‹å·: {torch.cuda.get_device_name(0)}")
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
    else:
        device = torch.device("cpu")
        print(f"ğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")
        print(f"  âš ï¸  CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPU")
        print(f"  è®­ç»ƒé€Ÿåº¦å¯èƒ½è¾ƒæ…¢ï¼Œè¯·è€å¿ƒç­‰å¾…")
    
    # ==================== æ•°æ®ç›®å½•é…ç½® ====================
    # ä½¿ç”¨å½“å‰é¡¹ç›®çš„æ•°æ®ç›®å½•
    data_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "..", "data")
    test_file = os.path.join(data_dir, "dev.csv")
    
    # ==================== åŠ è½½æ•°æ® ====================
    print("\n" + "="*70)
    print("ğŸ“Š åŠ è½½æ•°æ®")
    print("="*70)
    
    try:
        # åŠ è½½äº¤å‰éªŒè¯æ•°æ®ä½œä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†
        train_df, valid_df = load_cross_validation_data(data_dir)
        
        # åŠ è½½dev.csvä½œä¸ºæµ‹è¯•é›†
        test_df = load_test_data(test_file)
        
        # æ£€æŸ¥æ•°æ®æ ¼å¼
        print(f"\nğŸ“‹ æ•°æ®åˆ—å:")
        print(f"  è®­ç»ƒé›†: {list(train_df.columns)}")
        print(f"  éªŒè¯é›†: {list(valid_df.columns)}")
        print(f"  æµ‹è¯•é›†: {list(test_df.columns)}")
        
        # æ ‡ç­¾åˆ†å¸ƒ
        train_labels = train_df['label'].value_counts().sort_index()
        valid_labels = valid_df['label'].value_counts().sort_index()
        test_labels = test_df['label'].value_counts().sort_index()
        
        print(f"\nğŸ¯ æ ‡ç­¾åˆ†å¸ƒ:")
        for label in [0, 1]:
            train_count = train_labels.get(label, 0)
            valid_count = valid_labels.get(label, 0)
            test_count = test_labels.get(label, 0)
            label_name = "è´Ÿé¢" if label == 0 else "æ­£é¢"
            print(f"  {label_name} (æ ‡ç­¾={label}):")
            print(f"    è®­ç»ƒé›†: {train_count} æ¡ ({train_count/len(train_df)*100:.1f}%)")
            print(f"    éªŒè¯é›†: {valid_count} æ¡ ({valid_count/len(valid_df)*100:.1f}%)")
            print(f"    æµ‹è¯•é›†: {test_count} æ¡ ({test_count/len(test_df)*100:.1f}%)")
        
        # æ˜¾ç¤ºç¤ºä¾‹
        print(f"\nğŸ” æ•°æ®ç¤ºä¾‹:")
        for i in range(min(2, len(train_df))):
            text = train_df.iloc[i]['sentence']
            label = train_df.iloc[i]['label']
            sentiment = "è´Ÿé¢" if label == 0 else "æ­£é¢"
            print(f"  è®­ç»ƒé›†ç¤ºä¾‹ {i+1}:")
            print(f"    æ–‡æœ¬: {text[:60]}...")
            print(f"    æ ‡ç­¾: {label} ({sentiment})")
            print(f"    åˆ†è¯: {tokenize_chinese(text)[:12]}...")
            
    except FileNotFoundError as e:
        print(f"âŒ é”™è¯¯: {e}")
        print(f"è¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨")
        print(f"æ•°æ®ç›®å½•: {data_dir}")
        print(f"æµ‹è¯•æ–‡ä»¶: {test_file}")
        if os.path.exists(data_dir):
            print(f"æ•°æ®ç›®å½•å†…å®¹: {os.listdir(data_dir)}")
        return
    
    # ==================== æ„å»ºè¯æ±‡è¡¨ ====================
    print("\n" + "="*70)
    print("ğŸ“š æ„å»ºè¯æ±‡è¡¨")
    print("="*70)
    
    # åˆ›å»ºè¯æ±‡è¡¨
    min_freq = 2
    vocab = Vocabulary(min_freq=min_freq)
    
    # ä»è®­ç»ƒæ•°æ®æ„å»ºè¯æ±‡è¡¨ï¼ˆåªä½¿ç”¨è®­ç»ƒæ•°æ®ï¼‰
    train_texts = train_df['sentence'].astype(str).tolist()
    vocab.build_vocab(train_texts, tokenize_chinese)
    
    # è®¾ç½®ç‰¹æ®Štokençš„ç´¢å¼•
    unk_index = vocab.word2idx[vocab.unk_token]
    pad_index = vocab.word2idx[vocab.pad_token]
    vocab.set_default_index(unk_index)
    
    print(f"  UNKç´¢å¼•: {unk_index}")
    print(f"  PADç´¢å¼•: {pad_index}")
    
    # ==================== åˆ›å»ºæ•°æ®é›† ====================
    print("\n" + "="*70)
    print("ğŸ“ åˆ›å»ºæ•°æ®é›†")
    print("="*70)
    
    # æ•°æ®é›†å‚æ•°ï¼ˆä½¿ç”¨è¾ƒå°å‚æ•°ä»¥åŠ å¿«CPUè®­ç»ƒé€Ÿåº¦ï¼‰
    max_length = 128
    
    # åˆ›å»ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†
    train_dataset = JDDataset(
        train_df['sentence'].astype(str).tolist(),
        train_df['label'].astype(int).tolist(),
        vocab,
        max_length=max_length,
        tokenizer_fn=tokenize_chinese
    )
    
    valid_dataset = JDDataset(
        valid_df['sentence'].astype(str).tolist(),
        valid_df['label'].astype(int).tolist(),
        vocab,
        max_length=max_length,
        tokenizer_fn=tokenize_chinese
    )
    
    test_dataset = JDDataset(
        test_df['sentence'].astype(str).tolist(),
        test_df['label'].astype(int).tolist(),
        vocab,
        max_length=max_length,
        tokenizer_fn=tokenize_chinese
    )
    
    # ==================== åˆ›å»ºæ•°æ®åŠ è½½å™¨ ====================
    # ä½¿ç”¨è¾ƒå°çš„æ‰¹å¤„ç†å¤§å°ä»¥é€‚åº”CPUå†…å­˜
    batch_size = 16
    
    train_data_loader = get_data_loader(train_dataset, batch_size, pad_index, shuffle=True)
    valid_data_loader = get_data_loader(valid_dataset, batch_size, pad_index)
    test_data_loader = get_data_loader(test_dataset, batch_size, pad_index)
    
    print(f"âœ“ æœ€å¤§åºåˆ—é•¿åº¦: {max_length}")
    print(f"âœ“ æ‰¹å¤„ç†å¤§å°: {batch_size} (é€‚åº”CPUå†…å­˜)")
    print(f"âœ“ è®­ç»ƒæ‰¹æ¬¡æ•°: {len(train_data_loader)}")
    print(f"âœ“ éªŒè¯æ‰¹æ¬¡æ•°: {len(valid_data_loader)}")
    print(f"âœ“ æµ‹è¯•æ‰¹æ¬¡æ•°: {len(test_data_loader)}")
    
    # ==================== åˆ›å»ºæ¨¡å‹ ====================
    print("\n" + "="*70)
    print("ğŸ§  åˆ›å»ºLSTMæ¨¡å‹ (CPUä¼˜åŒ–ç‰ˆæœ¬)")
    print("="*70)
    
    # æ¨¡å‹å‚æ•°ï¼ˆä½¿ç”¨è¾ƒå°å‚æ•°ä»¥åŠ å¿«CPUè®­ç»ƒé€Ÿåº¦ï¼‰
    vocab_size = len(vocab)
    embedding_dim = 100  # å‡å°åµŒå…¥ç»´åº¦
    hidden_dim = 100     # å‡å°éšè—å±‚ç»´åº¦
    output_dim = 2
    n_layers = 2
    bidirectional = True
    dropout_rate = 0.3   # é™ä½dropout
    
    # åˆ›å»ºæ¨¡å‹
    model = LSTM(
        vocab_size,
        embedding_dim,
        hidden_dim,
        output_dim,
        n_layers,
        bidirectional,
        dropout_rate,
        pad_index,
    )
    
    # åˆå§‹åŒ–æƒé‡
    model.apply(initialize_weights)
    
    # å°†æ¨¡å‹ç§»åˆ°CPU
    model = model.to(device)
    
    print(f"âœ“ è¯æ±‡è¡¨å¤§å°: {vocab_size}")
    print(f"âœ“ åµŒå…¥ç»´åº¦: {embedding_dim} (CPUä¼˜åŒ–)")
    print(f"âœ“ éšè—å±‚ç»´åº¦: {hidden_dim} (CPUä¼˜åŒ–)")
    print(f"âœ“ LSTMå±‚æ•°: {n_layers}")
    print(f"âœ“ åŒå‘: {bidirectional}")
    print(f"âœ“ Dropout: {dropout_rate}")
    print(f"âœ“ æ¨¡å‹å‚æ•°é‡: {count_parameters(model):,}")
    print(f"âœ“ æ¨¡å‹å·²åŠ è½½åˆ°: {device}")
    
    # ==================== è®­ç»ƒé…ç½® ====================
    print("\n" + "="*70)
    print("âš™ï¸ è®­ç»ƒé…ç½®")
    print("="*70)
    
    # ä¼˜åŒ–å™¨ï¼ˆä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼‰
    lr = 0.001
    optimizer = optim.Adam(model.parameters(), lr=lr)
    
    # æŸå¤±å‡½æ•°
    criterion = nn.CrossEntropyLoss()
    criterion = criterion.to(device)
    
    print(f"âœ“ ä¼˜åŒ–å™¨: Adam (lr={lr})")
    print(f"âœ“ æŸå¤±å‡½æ•°: CrossEntropyLoss")
    print(f"âœ“ æ‰€æœ‰è®¡ç®—éƒ½åœ¨CPUä¸Šè¿›è¡Œ")
    
    # ==================== è®­ç»ƒå¾ªç¯ ====================
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ (CPUè®­ç»ƒï¼Œè¯·è€å¿ƒç­‰å¾…...)")
    print("="*70)
    
    n_epochs = 15  # å¢åŠ epochæ•°ï¼Œå› ä¸ºCPUè®­ç»ƒè¾ƒæ…¢ä½†ç¨³å®š
    best_valid_loss = float("inf")
    
    # è®°å½•è®­ç»ƒå†å²
    metrics = collections.defaultdict(list)
    
    for epoch in range(n_epochs):
        print(f"\nğŸ“ˆ Epoch {epoch+1}/{n_epochs}")
        
        # è®­ç»ƒ
        train_loss, train_acc = train(
            train_data_loader, model, criterion, optimizer, device
        )
        
        # è¯„ä¼°
        valid_loss, valid_acc = evaluate(
            valid_data_loader, model, criterion, device
        )
        
        # è®°å½•å†å²
        metrics["train_losses"].append(train_loss)
        metrics["train_accs"].append(train_acc)
        metrics["valid_losses"].append(valid_loss)
        metrics["valid_accs"].append(valid_acc)
        
        # æ‰“å°ç»“æœ
        print(f"epoch: {epoch+1}")
        print(f"train_loss: {train_loss:.3f}, train_acc: {train_acc:.3f}")
        print(f"valid_loss: {valid_loss:.3f}, valid_acc: {valid_acc:.3f}")
        
        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if valid_loss < best_valid_loss:
            best_valid_loss = valid_loss
            torch.save(model.state_dict(), "jd_lstm_best_cpu.pt")
            print(f"âœ“ ä¿å­˜æœ€ä½³æ¨¡å‹åˆ° jd_lstm_best_cpu.pt")
        
        # æ¯5ä¸ªepochä¿å­˜ä¸€æ¬¡æ£€æŸ¥ç‚¹
        if (epoch + 1) % 5 == 0:
            checkpoint_path = f"jd_lstm_epoch_{epoch+1}_cpu.pt"
            torch.save({
                'epoch': epoch,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'train_loss': train_loss,
                'valid_loss': valid_loss,
            }, checkpoint_path)
            print(f"âœ“ ä¿å­˜æ£€æŸ¥ç‚¹åˆ° {checkpoint_path}")
    
    # ==================== å¯è§†åŒ–ç»“æœ ====================
    print("\n" + "="*70)
    print("ğŸ“Š è®­ç»ƒç»“æœå¯è§†åŒ–")
    print("="*70)
    
    # åˆ›å»ºå¯è§†åŒ–ç›®å½•
    os.makedirs("results_cpu", exist_ok=True)
    
    fig = plt.figure(figsize=(10, 6))
    ax = fig.add_subplot(1, 1, 1)
    ax.plot(metrics["train_losses"], label="train loss", marker='o')
    ax.plot(metrics["valid_losses"], label="valid loss", marker='s')
    ax.set_xlabel("epoch")
    ax.set_ylabel("loss")
    ax.set_xticks(range(n_epochs))
    ax.legend()
    ax.grid()
    plt.title("è®­ç»ƒå’ŒéªŒè¯æŸå¤± (CPUè®­ç»ƒ)")
    plt.savefig("results_cpu/training_loss_cpu.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    fig = plt.figure(figsize=(10, 6))
    ax = fig.add_subplot(1, 1, 1)
    ax.plot(metrics["train_accs"], label="train accuracy", marker='o')
    ax.plot(metrics["valid_accs"], label="valid accuracy", marker='s')
    ax.set_xlabel("epoch")
    ax.set_ylabel("accuracy")
    ax.set_xticks(range(n_epochs))
    ax.legend()
    ax.grid()
    plt.title("è®­ç»ƒå’ŒéªŒè¯å‡†ç¡®ç‡ (CPUè®­ç»ƒ)")
    plt.savefig("results_cpu/training_accuracy_cpu.png", dpi=300, bbox_inches='tight')
    plt.show()
    
    print(f"âœ“ è®­ç»ƒæ›²çº¿å·²ä¿å­˜åˆ° results_cpu/training_loss_cpu.png å’Œ results_cpu/training_accuracy_cpu.png")
    
    # ==================== åŠ è½½æœ€ä½³æ¨¡å‹å¹¶åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼° ====================
    print("\n" + "="*70)
    print("ğŸ§ª åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°æ¨¡å‹")
    print("="*70)
    
    # åŠ è½½æœ€ä½³æ¨¡å‹
    model.load_state_dict(torch.load("jd_lstm_best_cpu.pt"))
    model.eval()
    
    # åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°
    test_loss, test_acc = evaluate(
        test_data_loader, model, criterion, device
    )
    
    print(f"ğŸ“Š æµ‹è¯•é›†ç»“æœ:")
    print(f"  æµ‹è¯•æŸå¤±: {test_loss:.4f}")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡: {test_acc:.4f}")
    print(f"  æµ‹è¯•å‡†ç¡®ç‡ç™¾åˆ†æ¯”: {test_acc*100:.2f}%")
    
    # ==================== æµ‹è¯•ç¤ºä¾‹ ====================
    print("\n" + "="*70)
    print("ğŸ” ç¤ºä¾‹é¢„æµ‹")
    print("="*70)
    
    # ä»æµ‹è¯•é›†ä¸­éšæœºé€‰æ‹©ä¸€äº›æ ·æœ¬
    np.random.seed(42)
    sample_indices = np.random.choice(len(test_df), 8, replace=False)
    
    print("æµ‹è¯•é›†ç¤ºä¾‹é¢„æµ‹:")
    print("-" * 70)
    for idx in sample_indices:
        text = test_df.iloc[idx]['sentence']
        true_label = test_df.iloc[idx]['label']
        true_sentiment = "æ­£é¢" if true_label == 1 else "è´Ÿé¢"
        
        predicted_class, confidence, neg_prob, pos_prob = predict_sentiment(
            text, model, tokenize_chinese, vocab, device, max_length
        )
        predicted_sentiment = "æ­£é¢" if predicted_class == 1 else "è´Ÿé¢"
        
        # æ£€æŸ¥é¢„æµ‹æ˜¯å¦æ­£ç¡®
        correct = "âœ“" if predicted_class == true_label else "âœ—"
        
        print(f"ğŸ“ æ–‡æœ¬: {text[:80]}...")
        print(f"  çœŸå®æƒ…æ„Ÿ: {true_sentiment} (æ ‡ç­¾={true_label})")
        print(f"  é¢„æµ‹æƒ…æ„Ÿ: {predicted_sentiment} {correct} (ç½®ä¿¡åº¦: {confidence:.2%})")
        print(f"  è´Ÿé¢æ¦‚ç‡: {neg_prob:.4f}, æ­£é¢æ¦‚ç‡: {pos_prob:.4f}")
        print()
    
    # è‡ªå®šä¹‰æµ‹è¯•æ–‡æœ¬
    test_texts = [
        "æ‰‹æœºè´¨é‡å¾ˆå¥½ï¼Œè¿è¡Œæµç•…ï¼Œéå¸¸æ»¡æ„ï¼",
        "ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†æ•´æ•´ä¸€å‘¨æ‰åˆ°è´§",
        "å®¢æœæ€åº¦å¾ˆå·®ï¼Œè§£å†³é—®é¢˜æ•ˆç‡ä½",
        "åŒ…è£…å¾ˆç²¾ç¾ï¼Œé€è´§é€Ÿåº¦ä¹Ÿå¾ˆå¿«",
        "å•†å“ä¸æè¿°ä¸ç¬¦ï¼Œæœ‰è´¨é‡é—®é¢˜",
        "æ€§ä»·æ¯”å¾ˆé«˜ï¼Œç‰©è¶…æ‰€å€¼",
        "å±å¹•æœ‰åˆ’ç—•ï¼Œå“æ§éœ€è¦åŠ å¼º",
        "æ“ä½œç®€å•ï¼Œé€‚åˆè€å¹´äººä½¿ç”¨"
    ]
    
    print("è‡ªå®šä¹‰æ–‡æœ¬é¢„æµ‹:")
    print("-" * 70)
    for text in test_texts:
        predicted_class, confidence, neg_prob, pos_prob = predict_sentiment(
            text, model, tokenize_chinese, vocab, device, max_length
        )
        sentiment = "æ­£é¢" if predicted_class == 1 else "è´Ÿé¢"
        print(f"ğŸ“ æ–‡æœ¬: {text}")
        print(f"  é¢„æµ‹æƒ…æ„Ÿ: {sentiment} (ç½®ä¿¡åº¦: {confidence:.2%})")
        print(f"  è´Ÿé¢æ¦‚ç‡: {neg_prob:.4f}, æ­£é¢æ¦‚ç‡: {pos_prob:.4f}")
        print()
    
    # ==================== ä¿å­˜ç»“æœ ====================
    print("\n" + "="*70)
    print("ğŸ’¾ ä¿å­˜æ¨¡å‹å’Œç»“æœ")
    print("="*70)
    
    # ä¿å­˜è¯æ±‡è¡¨
    vocab_data = {
        'word2idx': vocab.word2idx,
        'idx2word': vocab.idx2word,
        'unk_token': vocab.unk_token,
        'pad_token': vocab.pad_token,
        'min_freq': vocab.min_freq
    }
    torch.save(vocab_data, "jd_vocab_cpu.pt")
    
    # ä¿å­˜å®Œæ•´æ¨¡å‹ä¿¡æ¯
    model_info = {
        'model_state_dict': model.state_dict(),
        'vocab_size': vocab_size,
        'embedding_dim': embedding_dim,
        'hidden_dim': hidden_dim,
        'output_dim': output_dim,
        'n_layers': n_layers,
        'bidirectional': bidirectional,
        'dropout_rate': dropout_rate,
        'pad_index': pad_index,
        'max_length': max_length,
        'metrics': metrics,
        'test_results': {'loss': test_loss, 'accuracy': test_acc},
        'device': str(device),
        'training_info': 'CPUè®­ç»ƒç‰ˆæœ¬'
    }
    torch.save(model_info, "jd_lstm_full_cpu.pt")
    
    # ä¿å­˜æµ‹è¯•ç»“æœåˆ°CSV
    test_results = []
    for i in range(len(test_df)):
        text = test_df.iloc[i]['sentence']
        true_label = test_df.iloc[i]['label']
        
        predicted_class, confidence, neg_prob, pos_prob = predict_sentiment(
            text, model, tokenize_chinese, vocab, device, max_length
        )
        
        test_results.append({
            'text': text,
            'true_label': true_label,
            'predicted_label': predicted_class,
            'confidence': confidence,
            'neg_prob': neg_prob,
            'pos_prob': pos_prob,
            'correct': 1 if predicted_class == true_label else 0
        })
    
    results_df = pd.DataFrame(test_results)
    results_df.to_csv("test_predictions_cpu.csv", index=False, encoding='utf-8-sig')
    
    # è®¡ç®—å¹¶ä¿å­˜è¯„ä¼°æŒ‡æ ‡
    accuracy = results_df['correct'].mean()
    confusion_matrix = pd.crosstab(
        results_df['true_label'], 
        results_df['predicted_label'],
        rownames=['True'], 
        colnames=['Predicted']
    )
    
    with open("evaluation_results_cpu.txt", "w", encoding='utf-8') as f:
        f.write("äº¬ä¸œè¯„è®ºæƒ…æ„Ÿåˆ†æ - CPUè®­ç»ƒè¯„ä¼°ç»“æœ\n")
        f.write("=" * 50 + "\n\n")
        f.write(f"è®­ç»ƒè®¾å¤‡: {device}\n")
        f.write(f"æµ‹è¯•é›†å¤§å°: {len(test_df)}\n")
        f.write(f"æµ‹è¯•å‡†ç¡®ç‡: {accuracy:.4f} ({accuracy*100:.2f}%)\n")
        f.write(f"æµ‹è¯•æŸå¤±: {test_loss:.4f}\n\n")
        f.write("æ¨¡å‹å‚æ•°:\n")
        f.write(f"  åµŒå…¥ç»´åº¦: {embedding_dim}\n")
        f.write(f"  éšè—å±‚ç»´åº¦: {hidden_dim}\n")
        f.write(f"  LSTMå±‚æ•°: {n_layers}\n")
        f.write(f"  æ‰¹å¤„ç†å¤§å°: {batch_size}\n")
        f.write(f"  æœ€å¤§åºåˆ—é•¿åº¦: {max_length}\n\n")
        f.write("æ··æ·†çŸ©é˜µ:\n")
        f.write(str(confusion_matrix) + "\n\n")
        f.write("è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° test_predictions_cpu.csv\n")
    
    print(f"âœ“ è¯æ±‡è¡¨å·²ä¿å­˜åˆ° jd_vocab_cpu.pt")
    print(f"âœ“ å®Œæ•´æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜åˆ° jd_lstm_full_cpu.pt")
    print(f"âœ“ æµ‹è¯•é¢„æµ‹ç»“æœå·²ä¿å­˜åˆ° test_predictions_cpu.csv")
    print(f"âœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜åˆ° evaluation_results_cpu.txt")
    print(f"\nğŸ‰ CPUè®­ç»ƒå’Œè¯„ä¼°å®Œæˆï¼")
    print(f"ğŸ“ æ‰€æœ‰ç»“æœæ–‡ä»¶éƒ½ä¿å­˜åœ¨å½“å‰ç›®å½•ï¼Œå‰ç¼€ä¸º '_cpu'")
    print(f"â±ï¸  æ„Ÿè°¢æ‚¨çš„è€å¿ƒç­‰å¾…ï¼ŒCPUè®­ç»ƒå¯èƒ½éœ€è¦ä¸€äº›æ—¶é—´")

# ==================== è¿è¡Œä¸»å‡½æ•° ====================
if __name__ == "__main__":
    main()
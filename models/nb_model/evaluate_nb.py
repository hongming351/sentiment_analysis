# evaluate_nb.py - æœ´ç´ è´å¶æ–¯æ¨¡å‹è¯„ä¼°
import warnings
warnings.filterwarnings('ignore')

import os
import pandas as pd
import numpy as np
import pickle
import json
import time
from pathlib import Path
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import classification_report, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns

print("=" * 70)
print("äº¬ä¸œè¯„è®ºæƒ…æ„Ÿåˆ†æ - æœ´ç´ è´å¶æ–¯æ¨¡å‹è¯„ä¼°")
print("=" * 70)

# ==================== æ–‡æœ¬é¢„å¤„ç† ====================
def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    
    import re
    text = str(text).strip()
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?;\'"ã€]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def tokenize_chinese(text, use_jieba=True):
    """ä¸­æ–‡åˆ†è¯"""
    import jieba
    text = clean_text(text)
    if not text:
        return ""
    
    if use_jieba:
        tokens = jieba.lcut(text)
    else:
        tokens = list(text)
    
    tokens = [token.strip() for token in tokens if token.strip()]
    return ' '.join(tokens)

# ==================== åŠ è½½æµ‹è¯•æ•°æ® ====================
def load_test_data(data_dir=None):
    """åŠ è½½æµ‹è¯•æ•°æ®"""
    import os
    import pandas as pd
    
    if data_dir is None:
        # è‡ªåŠ¨è®¡ç®—ç»å¯¹è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(current_dir))
        data_dir = os.path.join(project_root, 'data')
    
    test_path = os.path.join(data_dir, "dev.csv")
    print(f"æµ‹è¯•æ–‡ä»¶è·¯å¾„: {test_path}")
    
    if not os.path.exists(test_path):
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æµ‹è¯•é›†æ–‡ä»¶: {test_path}")
    
    # åŠ è½½æ•°æ®
    test_df = pd.read_csv(test_path)
    print(f"åŸå§‹æµ‹è¯•é›†: {len(test_df)} æ¡")
    
    # æ£€æŸ¥å¹¶æ¸…ç†NaN
    original_len = len(test_df)
    test_df_clean = test_df.dropna(subset=['sentence', 'label'])
    
    print(f"æ¸…ç†åæµ‹è¯•é›†: {len(test_df_clean)} æ¡")
    print(f"ç§»é™¤äº† {original_len - len(test_df_clean)} æ¡åŒ…å«NaNçš„æ•°æ®")
    
    if len(test_df_clean) == 0:
        raise ValueError("æ¸…ç†åæ²¡æœ‰æœ‰æ•ˆæ•°æ®")
    
    # æ£€æŸ¥æ ‡ç­¾åˆ†å¸ƒ
    print("\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒç»Ÿè®¡:")
    label_counts = test_df_clean['label'].value_counts().sort_index()
    for label, count in label_counts.items():
        percentage = count / len(test_df_clean) * 100
        print(f"  æ ‡ç­¾ {label}: {count} æ¡ ({percentage:.1f}%)")
    
    return test_df_clean

# ==================== åŠ è½½æ¨¡å‹ ====================
def load_nb_models(model_dir=None, n_folds=5):
    """åŠ è½½æ‰€æœ‰æœ´ç´ è´å¶æ–¯æ¨¡å‹"""
    import os
    
    if model_dir is None:
        # è‡ªåŠ¨è®¡ç®—ç»å¯¹è·¯å¾„
        current_dir = os.path.dirname(os.path.abspath(__file__))
        project_root = os.path.dirname(os.path.dirname(current_dir))
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•å¤šä¸ªè·¯å¾„
        possible_dirs = [
            os.path.join(project_root, 'nb_models'),  # æ ¹ç›®å½•çš„nb_models
            os.path.join(current_dir, 'nb_models'),   # å½“å‰ç›®å½•çš„nb_models
            os.path.join(project_root, 'models', 'nb_model', 'nb_models'),
        ]
        
        model_dir = None
        for dir_path in possible_dirs:
            if os.path.exists(dir_path) and any(f'nb_fold_{i}.pkl' in os.listdir(dir_path) for i in range(n_folds)):
                model_dir = dir_path
                break
        
        if model_dir is None:
            model_dir = possible_dirs[0]  # é»˜è®¤ä½¿ç”¨ç¬¬ä¸€ä¸ª
    
    print(f"\nğŸ“‚ æ¨¡å‹ç›®å½•: {model_dir}")
    print(f"   ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(model_dir)}")
    
    if not os.path.exists(model_dir):
        raise FileNotFoundError(f"æ¨¡å‹ç›®å½•ä¸å­˜åœ¨: {model_dir}")
    
    # åˆ—å‡ºç›®å½•å†…å®¹
    try:
        files = os.listdir(model_dir)
        print(f"   ç›®å½•å†…å®¹ ({len(files)} ä¸ªæ–‡ä»¶):")
        for file in files:
            if file.endswith('.pkl'):
                print(f"     - {file}")
    except Exception as e:
        print(f"   æ— æ³•åˆ—å‡ºç›®å½•å†…å®¹: {e}")
    
    models = []
    loaded_count = 0
    
    for fold_idx in range(n_folds):
        model_path = os.path.join(model_dir, f"nb_fold_{fold_idx}.pkl")
        
        if not os.path.exists(model_path):
            print(f"âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
            continue
        
        try:
            with open(model_path, 'rb') as f:
                model = pickle.load(f)
                models.append(model)
            print(f"âœ“ åŠ è½½ç¬¬{fold_idx+1}æŠ˜æ¨¡å‹æˆåŠŸ")
            loaded_count += 1
        except Exception as e:
            print(f"âŒ åŠ è½½ç¬¬{fold_idx+1}æŠ˜æ¨¡å‹å¤±è´¥: {e}")
    
    if loaded_count == 0:
        raise FileNotFoundError("æœªèƒ½åŠ è½½ä»»ä½•æ¨¡å‹")
    
    print(f"\nâœ… æ€»å…±åŠ è½½äº† {loaded_count} ä¸ªæ¨¡å‹")
    return models

# ==================== è¯„ä¼°æ¨¡å‹ ====================
def evaluate_nb_models(nb_models, test_texts, test_labels):
    """è¯„ä¼°æœ´ç´ è´å¶æ–¯æ¨¡å‹æ€§èƒ½"""
    import numpy as np
    
    print(f"\nğŸ” æ•°æ®æ£€æŸ¥:")
    print(f"  æµ‹è¯•æ–‡æœ¬æ•°é‡: {len(test_texts)}")
    print(f"  æµ‹è¯•æ ‡ç­¾æ•°é‡: {len(test_labels)}")
    
    # ç¡®ä¿æ ‡ç­¾æ˜¯æ•´æ•°ç±»å‹
    test_labels = [int(label) for label in test_labels]
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    test_labels_np = np.array(test_labels)
    
    print(f"\nğŸ“Š æ ‡ç­¾åˆ†å¸ƒ:")
    unique_labels, counts = np.unique(test_labels_np, return_counts=True)
    for label, count in zip(unique_labels, counts):
        print(f"  æ ‡ç­¾ {label}: {count} æ¡ ({count/len(test_labels_np)*100:.1f}%)")
    
    # æ”¶é›†æ‰€æœ‰é¢„æµ‹
    all_predictions = []
    
    print(f"\nğŸ¤– å¼€å§‹é¢„æµ‹ ({len(nb_models)} ä¸ªæ¨¡å‹)...")
    for i, model in enumerate(nb_models):
        print(f"  æ¨¡å‹ {i+1}/{len(nb_models)} é¢„æµ‹ä¸­...")
        predictions = model.predict(test_texts)
        all_predictions.append(predictions)
    
    # è½¬æ¢ä¸ºnumpyæ•°ç»„
    all_predictions = np.array(all_predictions)
    
    # æŠ•ç¥¨é›†æˆ
    print("  è¿›è¡ŒæŠ•ç¥¨é›†æˆ...")
    ensemble_predictions = []
    
    for i in range(len(test_texts)):
        preds = all_predictions[:, i]
        # å–ä¼—æ•°
        unique, counts = np.unique(preds, return_counts=True)
        ensemble_predictions.append(unique[np.argmax(counts)])
    
    ensemble_predictions = np.array(ensemble_predictions)
    
    # ä½¿ç”¨ç¬¬ä¸€ä¸ªæ¨¡å‹çš„å•ä¸ªé¢„æµ‹
    single_predictions = all_predictions[0]
    
    # è®¡ç®—æŒ‡æ ‡
    print("\nğŸ“ˆ è®¡ç®—è¯„ä¼°æŒ‡æ ‡...")
    
    # é›†æˆæ¨¡å‹æŒ‡æ ‡
    ensemble_acc = accuracy_score(test_labels_np, ensemble_predictions)
    ensemble_precision = precision_score(test_labels_np, ensemble_predictions, average='weighted', zero_division=0)
    ensemble_recall = recall_score(test_labels_np, ensemble_predictions, average='weighted', zero_division=0)
    ensemble_f1 = f1_score(test_labels_np, ensemble_predictions, average='weighted', zero_division=0)
    
    # å•ä¸ªæ¨¡å‹æŒ‡æ ‡
    single_acc = accuracy_score(test_labels_np, single_predictions)
    single_precision = precision_score(test_labels_np, single_predictions, average='weighted', zero_division=0)
    single_recall = recall_score(test_labels_np, single_predictions, average='weighted', zero_division=0)
    single_f1 = f1_score(test_labels_np, single_predictions, average='weighted', zero_division=0)
    
    # åˆ†ç±»æŠ¥å‘Š
    print("\nğŸ“‹ é›†æˆæ¨¡å‹åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(test_labels_np, ensemble_predictions, digits=4))
    
    print("ğŸ“‹ å•ä¸ªæ¨¡å‹åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(test_labels_np, single_predictions, digits=4))
    
    # æ··æ·†çŸ©é˜µ
    ensemble_cm = confusion_matrix(test_labels_np, ensemble_predictions)
    single_cm = confusion_matrix(test_labels_np, single_predictions)
    
    return {
        'ensemble': {
            'accuracy': ensemble_acc,
            'precision': ensemble_precision,
            'recall': ensemble_recall,
            'f1': ensemble_f1,
            'predictions': ensemble_predictions,
            'confusion_matrix': ensemble_cm
        },
        'single': {
            'accuracy': single_acc,
            'precision': single_precision,
            'recall': single_recall,
            'f1': single_f1,
            'predictions': single_predictions,
            'confusion_matrix': single_cm
        },
        'test_labels': test_labels_np,
        'test_size': len(test_texts)
    }

# ==================== å¯è§†åŒ–ç»“æœ ====================
def plot_results(results, save_dir='nb_evaluation_results'):
    """å¯è§†åŒ–è¯„ä¼°ç»“æœ"""
    import os
    import matplotlib.pyplot as plt
    import seaborn as sns
    
    os.makedirs(save_dir, exist_ok=True)
    
    # è®¾ç½®ä¸­æ–‡å­—ä½“
    try:
        plt.rcParams['font.sans-serif'] = ['SimHei']
        plt.rcParams['axes.unicode_minus'] = False
    except:
        print("âš ï¸  è­¦å‘Š: ä¸­æ–‡å­—ä½“è®¾ç½®å¤±è´¥ï¼Œå›¾è¡¨å¯èƒ½æ— æ³•æ˜¾ç¤ºä¸­æ–‡")
    
    # 1. æ€§èƒ½æŒ‡æ ‡å¯¹æ¯”å›¾
    fig, ax = plt.subplots(2, 2, figsize=(12, 10))
    fig.suptitle('æœ´ç´ è´å¶æ–¯æ¨¡å‹æ€§èƒ½å¯¹æ¯”', fontsize=16)
    
    metrics = ['accuracy', 'precision', 'recall', 'f1']
    titles = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1åˆ†æ•°']
    
    for i, (metric, title) in enumerate(zip(metrics, titles)):
        row = i // 2
        col = i % 2
        
        ensemble_value = results['ensemble'][metric]
        single_value = results['single'][metric]
        
        ax[row, col].bar(['é›†æˆæ¨¡å‹', 'å•ä¸ªæ¨¡å‹'], [ensemble_value, single_value], 
                        color=['skyblue', 'lightcoral'])
        ax[row, col].set_title(title)
        ax[row, col].set_ylim([0, 1])
        
        # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
        for j, value in enumerate([ensemble_value, single_value]):
            ax[row, col].text(j, value + 0.02, f'{value:.4f}', 
                            ha='center', va='bottom', fontsize=10)
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'performance_comparison.png'), dpi=300, bbox_inches='tight')
    print(f"âœ“ æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜: {save_dir}/performance_comparison.png")
    
    # 2. æ··æ·†çŸ©é˜µ
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    fig.suptitle('æ··æ·†çŸ©é˜µå¯¹æ¯”', fontsize=16)
    
    # é›†æˆæ¨¡å‹æ··æ·†çŸ©é˜µ
    sns.heatmap(results['ensemble']['confusion_matrix'], 
                annot=True, fmt='d', cmap='Blues', ax=ax1)
    ax1.set_title('é›†æˆæ¨¡å‹')
    ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    ax1.set_ylabel('çœŸå®æ ‡ç­¾')
    
    # å•ä¸ªæ¨¡å‹æ··æ·†çŸ©é˜µ
    sns.heatmap(results['single']['confusion_matrix'], 
                annot=True, fmt='d', cmap='Reds', ax=ax2)
    ax2.set_title('å•ä¸ªæ¨¡å‹')
    ax2.set_xlabel('é¢„æµ‹æ ‡ç­¾')
    ax2.set_ylabel('çœŸå®æ ‡ç­¾')
    
    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'confusion_matrices.png'), dpi=300, bbox_inches='tight')
    print(f"âœ“ æ··æ·†çŸ©é˜µå›¾å·²ä¿å­˜: {save_dir}/confusion_matrices.png")
    
    # 3. é”™è¯¯åˆ†æå›¾
    errors = results['ensemble']['predictions'] != results['test_labels']
    error_indices = np.where(errors)[0]
    
    if len(error_indices) > 0:
        fig, ax = plt.subplots(figsize=(10, 6))
        
        error_labels_pred = results['ensemble']['predictions'][error_indices]
        error_labels_true = results['test_labels'][error_indices]
        
        error_pairs = list(zip(error_labels_true, error_labels_pred))
        unique_pairs, pair_counts = np.unique(error_pairs, axis=0, return_counts=True)
        
        # åˆ›å»ºæ ‡ç­¾
        pair_labels = [f'{true}â†’{pred}' for true, pred in unique_pairs]
        
        ax.bar(range(len(unique_pairs)), pair_counts, color='salmon')
        ax.set_title('é”™è¯¯ç±»å‹åˆ†æ (çœŸå®æ ‡ç­¾â†’é¢„æµ‹æ ‡ç­¾)')
        ax.set_xlabel('é”™è¯¯ç±»å‹')
        ax.set_ylabel('é”™è¯¯æ•°é‡')
        ax.set_xticks(range(len(unique_pairs)))
        ax.set_xticklabels(pair_labels, rotation=45, ha='right')
        
        # æ·»åŠ æ•°é‡æ ‡ç­¾
        for i, count in enumerate(pair_counts):
            ax.text(i, count + 0.5, str(count), ha='center', va='bottom')
        
        plt.tight_layout()
        plt.savefig(os.path.join(save_dir, 'error_analysis.png'), dpi=300, bbox_inches='tight')
        print(f"âœ“ é”™è¯¯åˆ†æå›¾å·²ä¿å­˜: {save_dir}/error_analysis.png")
    
    plt.close('all')
    print(f"\nâœ… æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {save_dir}")

# ==================== ä¸»å‡½æ•° ====================
# ==================== ä¸»å‡½æ•° ====================
def main():
    import os
    
    # é…ç½®å‚æ•°
    config = {
        'data_dir': None,  # è‡ªåŠ¨æ£€æµ‹
        'model_dir': None,  # è‡ªåŠ¨æ£€æµ‹
        'text_column': 'sentence',
        'label_column': 'label',
        'n_folds': 5,
        'results_dir': 'nb_evaluation_results',
        'seed': 42
    }
    
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    for key, value in config.items():
        if value is not None:
            print(f"  {key}: {value}")
    
    # åˆ›å»ºç»“æœç›®å½•
    os.makedirs(config['results_dir'], exist_ok=True)
    print(f"âœ“ åˆ›å»ºç»“æœç›®å½•: {config['results_dir']}")
    
    # åŠ è½½æµ‹è¯•æ•°æ®
    print("\n" + "="*70)
    print("ğŸ“Š åŠ è½½æµ‹è¯•æ•°æ®")
    print("="*70)
    
    try:
        test_df = load_test_data(config['data_dir'])
        print(f"âœ“ æµ‹è¯•é›†åŠ è½½å®Œæˆ: {len(test_df)} æ¡æ•°æ®")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        return
    
    # æ–‡æœ¬é¢„å¤„ç†
    print("\nğŸ”„ æ–‡æœ¬é¢„å¤„ç†ä¸­...")
    test_texts = test_df[config['text_column']].apply(tokenize_chinese).tolist()
    test_labels = test_df[config['label_column']].tolist()
    
    print(f"  é¢„å¤„ç†åæ–‡æœ¬æ•°é‡: {len(test_texts)}")
    print(f"  é¢„å¤„ç†åæ ‡ç­¾æ•°é‡: {len(test_labels)}")
    
    # åŠ è½½æ¨¡å‹
    print("\n" + "="*70)
    print("ğŸ¤– åŠ è½½æœ´ç´ è´å¶æ–¯æ¨¡å‹")
    print("="*70)
    
    try:
        nb_models = load_nb_models(config['model_dir'], config['n_folds'])
        print(f"âœ“ åŠ è½½äº† {len(nb_models)} ä¸ªæœ´ç´ è´å¶æ–¯æ¨¡å‹")
    except Exception as e:
        print(f"âŒ é”™è¯¯: {e}")
        print("è¯·å…ˆè¿è¡Œ train_nb.py è®­ç»ƒæ¨¡å‹")
        return
    
    # è¯„ä¼°æ¨¡å‹
    print("\n" + "="*70)
    print("ğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    print("="*70)
    
    results = evaluate_nb_models(nb_models, test_texts, test_labels)
    
    # æ˜¾ç¤ºç»“æœ
    print("\n" + "="*70)
    print("ğŸ“Š è¯„ä¼°ç»“æœæ€»ç»“")
    print("="*70)
    
    print(f"\nğŸ¯ é›†æˆæ¨¡å‹ (æŠ•ç¥¨æ³•):")
    print(f"  å‡†ç¡®ç‡: {results['ensemble']['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['ensemble']['precision']:.4f}")
    print(f"  å¬å›ç‡: {results['ensemble']['recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['ensemble']['f1']:.4f}")
    
    print(f"\nğŸ¯ å•ä¸ªæ¨¡å‹ (ç¬¬1æŠ˜):")
    print(f"  å‡†ç¡®ç‡: {results['single']['accuracy']:.4f}")
    print(f"  ç²¾ç¡®ç‡: {results['single']['precision']:.4f}")
    print(f"  å¬å›ç‡: {results['single']['recall']:.4f}")
    print(f"  F1åˆ†æ•°: {results['single']['f1']:.4f}")
    
    print(f"\nğŸ“ˆ æ€§èƒ½æå‡:")
    accuracy_improvement = results['ensemble']['accuracy'] - results['single']['accuracy']
    f1_improvement = results['ensemble']['f1'] - results['single']['f1']
    print(f"  å‡†ç¡®ç‡æå‡: {accuracy_improvement:.4f}")
    print(f"  F1åˆ†æ•°æå‡: {f1_improvement:.4f}")
    
    # ä¿å­˜ç»“æœ
    print("\nğŸ’¾ ä¿å­˜è¯„ä¼°ç»“æœ...")
    
    # è½¬æ¢å‡½æ•°ï¼šå°†numpyå¯¹è±¡è½¬æ¢ä¸ºPythonåŸç”Ÿç±»å‹
    def convert_for_json(obj):
        """é€’å½’è½¬æ¢å¯¹è±¡ä¸ºJSONå¯åºåˆ—åŒ–çš„æ ¼å¼"""
        if isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, dict):
            return {key: convert_for_json(value) for key, value in obj.items()}
        elif isinstance(obj, list):
            return [convert_for_json(item) for item in obj]
        else:
            return obj
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®
    results_to_save = {
        'config': config,
        'metrics': {
            'ensemble': {
                'accuracy': float(results['ensemble']['accuracy']),
                'precision': float(results['ensemble']['precision']),
                'recall': float(results['ensemble']['recall']),
                'f1': float(results['ensemble']['f1']),
                'confusion_matrix': convert_for_json(results['ensemble']['confusion_matrix'])
            },
            'single': {
                'accuracy': float(results['single']['accuracy']),
                'precision': float(results['single']['precision']),
                'recall': float(results['single']['recall']),
                'f1': float(results['single']['f1']),
                'confusion_matrix': convert_for_json(results['single']['confusion_matrix'])
            }
        },
        'test_info': {
            'size': results['test_size'],
            'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
        },
        'performance_improvement': {
            'accuracy_improvement': float(accuracy_improvement),
            'f1_improvement': float(f1_improvement)
        }
    }
    
    results_file = os.path.join(config['results_dir'], 'nb_evaluation_results.json')
    with open(results_file, 'w', encoding='utf-8') as f:
        json.dump(results_to_save, f, ensure_ascii=False, indent=2)
    
    print(f"âœ“ è¯„ä¼°ç»“æœå·²ä¿å­˜: {results_file}")
    
    # å¯è§†åŒ–
    print("\nğŸ¨ ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨...")
    try:
        plot_results(results, save_dir=config['results_dir'])
    except Exception as e:
        print(f"âš ï¸  å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
    
    print("\n" + "="*70)
    print("âœ… æœ´ç´ è´å¶æ–¯æ¨¡å‹è¯„ä¼°å®Œæˆ")
    print("="*70)
    
    print(f"\nğŸ“ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  è¯„ä¼°ç»“æœ: {config['results_dir']}/nb_evaluation_results.json")
    print(f"  æ€§èƒ½å¯¹æ¯”å›¾: {config['results_dir']}/performance_comparison.png")
    print(f"  æ··æ·†çŸ©é˜µå›¾: {config['results_dir']}/confusion_matrices.png")
    print(f"  é”™è¯¯åˆ†æå›¾: {config['results_dir']}/error_analysis.png")
    
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥å»ºè®®:")
    print("  1. æŸ¥çœ‹ç”Ÿæˆçš„å›¾è¡¨åˆ†ææ¨¡å‹æ€§èƒ½")
    print("  2. å¯¹æ¯”SVMå’Œæœ´ç´ è´å¶æ–¯æ¨¡å‹çš„ç»“æœ")
    print("  3. æ ¹æ®é”™è¯¯åˆ†æä¼˜åŒ–æ¨¡å‹æˆ–æ•°æ®")

if __name__ == "__main__":
    main()

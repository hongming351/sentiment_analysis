# train_nb.py - æœ´ç´ è´å¶æ–¯æ¨¡å‹5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ
import warnings
warnings.filterwarnings('ignore')

import os
import pandas as pd
import numpy as np
import pickle
import time
import json
from pathlib import Path
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, classification_report
from tqdm import tqdm

print("=" * 70)
print("äº¬ä¸œè¯„è®ºæƒ…æ„Ÿåˆ†æ - æœ´ç´ è´å¶æ–¯æ¨¡å‹5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ")
print("=" * 70)

# ==================== æ–‡æœ¬é¢„å¤„ç† ====================
def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""
    
    import re
    text = str(text).strip()
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?;\'"ã€]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def tokenize_chinese(text, use_jieba=True):
    """ä¸­æ–‡åˆ†è¯"""
    import jieba
    text = clean_text(text)
    if not text:
        return ""
    
    if use_jieba:
        tokens = jieba.lcut(text)
    else:
        tokens = list(text)
    
    tokens = [token.strip() for token in tokens if token.strip()]
    return ' '.join(tokens)

# ==================== åŠ è½½æ•°æ® ====================
def load_fold_data(data_dir='data', n_folds=5):
    """åŠ è½½äº¤å‰éªŒè¯æ•°æ®"""
    folds = []
    
    print(f"\nğŸ“‚ åŠ è½½æ•°æ®è¯¦ç»†ä¿¡æ¯:")
    print(f"  æ•°æ®ç›®å½•: {data_dir}")
    print(f"  ç›®å½•æ˜¯å¦å­˜åœ¨: {os.path.exists(data_dir)}")
    
    if not os.path.exists(data_dir):
        raise FileNotFoundError(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
    
    # åˆ—å‡ºæ•°æ®æ–‡ä»¶
    csv_files = [f for f in os.listdir(data_dir) if f.endswith('.csv')]
    print(f"  æ‰¾åˆ° {len(csv_files)} ä¸ªCSVæ–‡ä»¶")
    print(f"  æ–‡ä»¶åˆ—è¡¨: {csv_files[:10]}...")
    
    for fold_idx in range(n_folds):
        train_path = Path(data_dir) / f"train_fold_{fold_idx}.csv"
        val_path = Path(data_dir) / f"val_fold_{fold_idx}.csv"
        
        print(f"\n  å¤„ç†ç¬¬{fold_idx}æŠ˜:")
        print(f"    è®­ç»ƒæ–‡ä»¶: {train_path}")
        print(f"    æ˜¯å¦å­˜åœ¨: {train_path.exists()}")
        print(f"    éªŒè¯æ–‡ä»¶: {val_path}")
        print(f"    æ˜¯å¦å­˜åœ¨: {val_path.exists()}")
        
        if not train_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç¬¬{fold_idx}æŠ˜è®­ç»ƒæ•°æ®æ–‡ä»¶: {train_path}")
        
        if not val_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°ç¬¬{fold_idx}æŠ˜éªŒè¯æ•°æ®æ–‡ä»¶: {val_path}")
        
        train_df = pd.read_csv(train_path)
        val_df = pd.read_csv(val_path)
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        print(f"    è®­ç»ƒé›†å¤§å°: {len(train_df)} è¡Œ")
        print(f"    éªŒè¯é›†å¤§å°: {len(val_df)} è¡Œ")
        
        # æ£€æŸ¥NaNå€¼
        train_nan = train_df.isna().sum().sum()
        val_nan = val_df.isna().sum().sum()
        if train_nan > 0 or val_nan > 0:
            print(f"    è­¦å‘Š: è®­ç»ƒé›†æœ‰ {train_nan} ä¸ªNaNï¼ŒéªŒè¯é›†æœ‰ {val_nan} ä¸ªNaN")
            # æ¸…ç†NaN
            train_df = train_df.dropna(subset=['sentence', 'label'])
            val_df = val_df.dropna(subset=['sentence', 'label'])
            print(f"    æ¸…ç†å: è®­ç»ƒé›† {len(train_df)} è¡Œï¼ŒéªŒè¯é›† {len(val_df)} è¡Œ")
        
        folds.append({
            'fold': fold_idx,
            'train': train_df,
            'val': val_df
        })
    
    # åŠ è½½æµ‹è¯•é›†
    test_path = Path(data_dir) / "dev.csv"
    print(f"\n  æµ‹è¯•é›†æ–‡ä»¶: {test_path}")
    print(f"  æ˜¯å¦å­˜åœ¨: {test_path.exists()}")
    
    if not test_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°æµ‹è¯•é›†æ–‡ä»¶: {test_path}")
    
    test_df = pd.read_csv(test_path)
    print(f"  æµ‹è¯•é›†å¤§å°: {len(test_df)} è¡Œ")
    
    return folds, test_df

# ==================== è®­ç»ƒå•æŠ˜æœ´ç´ è´å¶æ–¯ ====================
def train_nb_fold(fold_idx, train_df, val_df, config):
    """è®­ç»ƒå•ä¸ªæœ´ç´ è´å¶æ–¯æ¨¡å‹"""
    print(f"\nğŸ“Š ç¬¬ {fold_idx+1}/{config['n_folds']} æŠ˜è®­ç»ƒ")
    print("-" * 50)
    
    # æ–‡æœ¬é¢„å¤„ç†
    print("æ–‡æœ¬é¢„å¤„ç†ä¸­...")
    train_texts = train_df[config['text_column']].apply(tokenize_chinese).tolist()
    train_labels = train_df[config['label_column']].astype(int).tolist()
    
    val_texts = val_df[config['text_column']].apply(tokenize_chinese).tolist()
    val_labels = val_df[config['label_column']].astype(int).tolist()
    
    # åˆ›å»ºæœ´ç´ è´å¶æ–¯æ¨¡å‹
    print("åˆ›å»ºæœ´ç´ è´å¶æ–¯æ¨¡å‹ä¸­...")
    
    if config['vectorizer_type'] == 'tfidf':
        vectorizer = TfidfVectorizer(
            max_features=config['max_features'],
            ngram_range=(1, config['ngram_range']),
            min_df=config['min_df'],
            max_df=config['max_df'],
            sublinear_tf=True
        )
    else:  # count vectorizer
        vectorizer = CountVectorizer(
            max_features=config['max_features'],
            ngram_range=(1, config['ngram_range']),
            min_df=config['min_df'],
            max_df=config['max_df']
        )
    
    nb_pipeline = Pipeline([
        ('vectorizer', vectorizer),
        ('classifier', MultinomialNB(
            alpha=config['alpha'],
            fit_prior=config['fit_prior']
        ))
    ])
    
    # è®­ç»ƒ
    print("è®­ç»ƒæ¨¡å‹ä¸­...")
    start_time = time.time()
    nb_pipeline.fit(train_texts, train_labels)
    train_time = time.time() - start_time
    
    # éªŒè¯é›†è¯„ä¼°
    print("éªŒè¯é›†è¯„ä¼°ä¸­...")
    val_predictions = nb_pipeline.predict(val_texts)
    val_accuracy = accuracy_score(val_labels, val_predictions)
    
    # ä¿å­˜æ¨¡å‹
    model_dir = "nb_models"
    os.makedirs(model_dir, exist_ok=True)
    model_path = os.path.join(model_dir, f"nb_fold_{fold_idx}.pkl")
    
    with open(model_path, 'wb') as f:
        pickle.dump(nb_pipeline, f)
    
    print(f"âœ“ æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    print(f"  è®­ç»ƒæ—¶é—´: {train_time:.2f}ç§’")
    print(f"  éªŒè¯é›†å‡†ç¡®ç‡: {val_accuracy:.4f}")
    
    return {
        'fold': fold_idx,
        'model_path': model_path,
        'train_time': train_time,
        'val_accuracy': val_accuracy
    }

# ==================== ä¸»å‡½æ•° ====================
def main():
    import os
    
    # è·å–å½“å‰è„šæœ¬çš„ç»å¯¹è·¯å¾„
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # è®¡ç®—é¡¹ç›®æ ¹ç›®å½•
    project_root = os.path.dirname(os.path.dirname(current_dir))
    # æ„å»ºæ­£ç¡®çš„æ•°æ®è·¯å¾„
    data_dir = os.path.join(project_root, 'data')
    
    # é…ç½®å‚æ•°
    config = {
        'data_dir': data_dir,
        'text_column': 'sentence',
        'label_column': 'label',
        'n_folds': 5,
        
        # ç‰¹å¾å·¥ç¨‹å‚æ•°
        'vectorizer_type': 'tfidf',  # 'tfidf' æˆ– 'count'
        'max_features': 10000,
        'ngram_range': 2,
        'min_df': 2,
        'max_df': 0.95,
        
        # æœ´ç´ è´å¶æ–¯å‚æ•°
        'alpha': 1.0,  # æ‹‰æ™®æ‹‰æ–¯å¹³æ»‘å‚æ•°
        'fit_prior': True,  # æ˜¯å¦å­¦ä¹ å…ˆéªŒæ¦‚ç‡
        
        # å…¶ä»–
        'seed': 42
    }
    
    print(f"\nğŸ“‹ é…ç½®ä¿¡æ¯:")
    print(f"  å½“å‰è„šæœ¬ä½ç½®: {current_dir}")
    print(f"  é¡¹ç›®æ ¹ç›®å½•: {project_root}")
    print(f"  æ•°æ®ç›®å½•: {config['data_dir']}")
    print(f"  å‘é‡åŒ–å™¨ç±»å‹: {config['vectorizer_type']}")
    print(f"  æœ€å¤§ç‰¹å¾æ•°: {config['max_features']}")
    print(f"  n-gramèŒƒå›´: 1-{config['ngram_range']}")
    print(f"  å¹³æ»‘å‚æ•°alpha: {config['alpha']}")
    
    # åŠ è½½æ•°æ®
    print("\n" + "="*70)
    print("ğŸ“Š åŠ è½½æ•°æ®")
    print("="*70)
    
    try:
        folds, test_df = load_fold_data(config['data_dir'], config['n_folds'])
        print(f"\nâœ“ æˆåŠŸåŠ è½½ {len(folds)} æŠ˜äº¤å‰éªŒè¯æ•°æ®")
        print(f"âœ“ æµ‹è¯•é›†: {len(test_df)} æ¡è¯„è®º")
    except FileNotFoundError as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        print("è¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®ã€‚")
        return
    
    # 5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ")
    print("="*70)
    
    fold_results = []
    
    for fold_idx in range(config['n_folds']):
        fold_data = folds[fold_idx]
        result = train_nb_fold(
            fold_idx=fold_idx,
            train_df=fold_data['train'],
            val_df=fold_data['val'],
            config=config
        )
        fold_results.append(result)
    
    # ä¿å­˜è®­ç»ƒé…ç½®å’Œç»“æœ
    results_dir = "nb_results"
    os.makedirs(results_dir, exist_ok=True)
    
    # ä¿å­˜é…ç½®
    with open(os.path.join(results_dir, 'nb_config.json'), 'w', encoding='utf-8') as f:
        json.dump(config, f, ensure_ascii=False, indent=2)
    
    # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
    val_accuracies = [r['val_accuracy'] for r in fold_results]
    
    # ä¿å­˜è®­ç»ƒç»“æœæ‘˜è¦
    training_summary = {
        'fold_results': fold_results,
        'total_training_time': sum([r['train_time'] for r in fold_results]),
        'avg_training_time': np.mean([r['train_time'] for r in fold_results]),
        'val_accuracies': val_accuracies,
        'mean_val_accuracy': np.mean(val_accuracies),
        'std_val_accuracy': np.std(val_accuracies),
        'timestamp': time.strftime('%Y-%m-%d %H:%M:%S')
    }
    
    with open(os.path.join(results_dir, 'nb_training_summary.json'), 'w', encoding='utf-8') as f:
        json.dump(training_summary, f, ensure_ascii=False, indent=2)
    
    print("\n" + "="*70)
    print("âœ… æœ´ç´ è´å¶æ–¯è®­ç»ƒå®Œæˆæ€»ç»“")
    print("="*70)
    
    print(f"\nğŸ“Š è®­ç»ƒç»Ÿè®¡:")
    print(f"  æ€»è®­ç»ƒæ—¶é—´: {training_summary['total_training_time']:.2f}ç§’")
    print(f"  å¹³å‡æ¯æŠ˜è®­ç»ƒæ—¶é—´: {training_summary['avg_training_time']:.2f}ç§’")
    print(f"  éªŒè¯é›†å‡†ç¡®ç‡: {training_summary['mean_val_accuracy']:.4f} (Â±{training_summary['std_val_accuracy']:.4f})")
    print(f"  ç”Ÿæˆçš„æ¨¡å‹: nb_models/nb_fold_0.pkl åˆ° nb_fold_4.pkl")
    
    print(f"\nğŸ“ˆ å„æŠ˜éªŒè¯é›†å‡†ç¡®ç‡:")
    for i, result in enumerate(fold_results):
        print(f"  ç¬¬{i+1}æŠ˜: {result['val_accuracy']:.4f}")
    
    print(f"\nğŸš€ ä¸‹ä¸€æ­¥:")
    print(f"  è¿è¡Œ evaluate_nb.py è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    print(f"  è¿è¡Œ python evaluate_nb.py")

if __name__ == "__main__":
    main()
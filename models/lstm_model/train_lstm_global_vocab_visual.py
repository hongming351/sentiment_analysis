# train_lstm_global_vocab_visual.py - ä¿®å¤è¯æ±‡è¡¨é—®é¢˜ + å®Œæ•´å¯è§†åŒ– + æ­£ç¡®è·¯å¾„
import warnings
warnings.filterwarnings('ignore')

import os
import sys
import pandas as pd
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import re
import jieba
import matplotlib.pyplot as plt
import matplotlib
from collections import Counter
from tqdm import tqdm
import json
import time
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, classification_report
import pickle
import seaborn as sns
from matplotlib import cm

matplotlib.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'STXihei']
matplotlib.rcParams['axes.unicode_minus'] = False

print("=" * 70)
print("äº¬ä¸œè¯„è®ºæƒ…æ„Ÿåˆ†æ - LSTMæ¨¡å‹5æŠ˜äº¤å‰éªŒè¯ï¼ˆä¿®å¤è¯æ±‡è¡¨é—®é¢˜ + å®Œæ•´å¯è§†åŒ– + æ­£ç¡®è·¯å¾„ï¼‰")
print("=" * 70)

# ==================== 1. è®¾ç½®éšæœºç§å­ ====================
def set_seed(seed=42):
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

set_seed(1234)

# ==================== 2. æ–‡æœ¬é¢„å¤„ç†å‡½æ•° ====================
def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬"""
    if pd.isna(text):
        return ""

    text = str(text).strip()
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'http\S+|www\S+|https\S+', '', text)
    text = re.sub(r'\S+@\S+', '', text)
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š,.!?;\'"ã€]', ' ', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def tokenize_chinese(text, use_jieba=True):
    """ä¸­æ–‡åˆ†è¯"""
    text = clean_text(text)
    if not text:
        return []

    if use_jieba:
        tokens = jieba.lcut(text)
    else:
        tokens = list(text)

    tokens = [token.strip() for token in tokens if token.strip()]
    return tokens

# ==================== 3. è‡ªå®šä¹‰è¯æ±‡è¡¨ç±» ====================
class Vocabulary:
    def __init__(self, min_freq=2, max_size=None):
        self.min_freq = min_freq
        self.max_size = max_size
        self.word2idx = {}
        self.idx2word = {}
        self.word_freq = Counter()

        self.pad_token = '<PAD>'
        self.unk_token = '<UNK>'
        self.sos_token = '<SOS>'
        self.eos_token = '<EOS>'

        self.add_word(self.pad_token)
        self.add_word(self.unk_token)
        self.add_word(self.sos_token)
        self.add_word(self.eos_token)

    def get_vocab_data(self):
        """è·å–å¯åºåˆ—åŒ–çš„è¯æ±‡è¡¨æ•°æ®"""
        return {
            'word2idx': self.word2idx,
            'idx2word': {int(k): v for k, v in self.idx2word.items()},
            'word_freq': dict(self.word_freq),
            'min_freq': self.min_freq,
            'max_size': self.max_size,
            'special_tokens': {
                'pad': self.pad_token,
                'unk': self.unk_token,
                'sos': self.sos_token,
                'eos': self.eos_token
            }
        }

    def add_word(self, word):
        if word not in self.word2idx:
            idx = len(self.word2idx)
            self.word2idx[word] = idx
            self.idx2word[idx] = word

    def build_vocab(self, texts, tokenizer_fn):
        """ä»æ–‡æœ¬æ„å»ºè¯æ±‡è¡¨"""
        print("æ„å»ºè¯æ±‡è¡¨ä¸­...")

        all_words = []
        for text in tqdm(texts, desc="å¤„ç†æ–‡æœ¬"):
            tokens = tokenizer_fn(text)
            all_words.extend(tokens)
            self.word_freq.update(tokens)

        filtered_words = []
        for word, freq in self.word_freq.items():
            if freq >= self.min_freq:
                filtered_words.append((word, freq))

        filtered_words.sort(key=lambda x: x[1], reverse=True)

        if self.max_size:
            filtered_words = filtered_words[:self.max_size - len(self.word2idx)]

        for word, _ in filtered_words:
            self.add_word(word)

        print(f"è¯æ±‡è¡¨æ„å»ºå®Œæˆï¼Œå¤§å°: {len(self)}")

    def __len__(self):
        return len(self.word2idx)

    def word_to_index(self, word):
        return self.word2idx.get(word, self.word2idx[self.unk_token])

    def index_to_word(self, idx):
        return self.idx2word.get(idx, self.unk_token)

    def encode(self, tokens, add_special_tokens=False, max_len=None):
        indices = []

        if add_special_tokens:
            indices.append(self.word2idx[self.sos_token])

        for token in tokens:
            indices.append(self.word_to_index(token))

        if add_special_tokens:
            indices.append(self.word2idx[self.eos_token])

        if max_len:
            if len(indices) > max_len:
                indices = indices[:max_len]
            else:
                indices = indices + [self.word2idx[self.pad_token]] * (max_len - len(indices))

        return indices

    def save(self, filepath):
        """ä¿å­˜è¯æ±‡è¡¨"""
        vocab_data = self.get_vocab_data()
        torch.save(vocab_data, filepath)
        print(f"è¯æ±‡è¡¨å·²ä¿å­˜åˆ°: {filepath}")

# ==================== 4. æ•°æ®é›†ç±» ====================
class JDDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_len=128, tokenizer_fn=None):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_len = max_len

        if tokenizer_fn is None:
            self.tokenizer_fn = tokenize_chinese
        else:
            self.tokenizer_fn = tokenizer_fn

        print("é¢„å¤„ç†æ•°æ®ä¸­...")
        self.encoded_texts = []
        self.lengths = []

        valid_indices = []

        for i, text in enumerate(tqdm(texts, desc="ç¼–ç æ–‡æœ¬")):
            tokens = self.tokenizer_fn(text)
            if len(tokens) == 0:
                continue

            encoded = self.vocab.encode(tokens, max_len=self.max_len)
            self.encoded_texts.append(encoded)
            self.lengths.append(min(len(tokens), self.max_len))
            valid_indices.append(i)

        self.labels = [labels[i] for i in valid_indices]
        self.texts = [texts[i] for i in valid_indices]

        if len(texts) - len(self.texts) > 0:
            print(f"  ç§»é™¤äº† {len(texts) - len(self.texts)} ä¸ªç©ºæ–‡æœ¬æ ·æœ¬")

        print(f"  æœ‰æ•ˆæ ·æœ¬æ•°: {len(self)}")

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        return {
            'text': torch.tensor(self.encoded_texts[idx], dtype=torch.long),
            'length': torch.tensor(self.lengths[idx], dtype=torch.long),
            'label': torch.tensor(self.labels[idx], dtype=torch.long),
            'original': self.texts[idx]
        }

    @classmethod
    def from_dataframe(cls, df, vocab, max_length=128,
                    text_col='sentence', label_col='label',
                    tokenizer_fn=None):
        print(f"ä»DataFrameåˆ›å»ºæ•°æ®é›†ï¼ŒåŸå§‹æ ·æœ¬æ•°: {len(df)}")

        df_clean = df.dropna(subset=[text_col, label_col])
        removed_count = len(df) - len(df_clean)
        if removed_count > 0:
            print(f"  ç§»é™¤äº† {removed_count} ä¸ªåŒ…å«NaNçš„æ ·æœ¬")

        df_clean[label_col] = pd.to_numeric(df_clean[label_col], errors='coerce')
        df_clean = df_clean.dropna(subset=[label_col])
        df_clean[label_col] = df_clean[label_col].astype(int)

        print(f"  æ¸…ç†åæ ·æœ¬æ•°: {len(df_clean)}")

        texts = df_clean[text_col].astype(str).tolist()
        labels = df_clean[label_col].tolist()

        return cls(texts, labels, vocab, max_length, tokenizer_fn)

# ==================== 5. LSTMæ¨¡å‹ ====================
class LSTMSentiment(nn.Module):
    def __init__(self, vocab_size, embed_dim=128, hidden_dim=256,
                 output_dim=2, n_layers=2, dropout=0.5, bidirectional=True):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)

        self.lstm = nn.LSTM(
            embed_dim,
            hidden_dim,
            num_layers=n_layers,
            bidirectional=bidirectional,
            dropout=dropout if n_layers > 1 else 0,
            batch_first=True
        )

        lstm_output_dim = hidden_dim * 2 if bidirectional else hidden_dim
        self.fc = nn.Linear(lstm_output_dim, output_dim)
        self.dropout = nn.Dropout(dropout)

        self._init_weights()

    def _init_weights(self):
        nn.init.uniform_(self.embedding.weight, -0.1, 0.1)
        self.embedding.weight.data[0] = torch.zeros(self.embedding.embedding_dim)

        for name, param in self.lstm.named_parameters():
            if 'weight' in name:
                nn.init.orthogonal_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)

        nn.init.xavier_uniform_(self.fc.weight)
        nn.init.constant_(self.fc.bias, 0)

    def forward(self, text, text_lengths):
        embedded = self.dropout(self.embedding(text))

        packed_embedded = nn.utils.rnn.pack_padded_sequence(
            embedded, text_lengths.cpu(), batch_first=True, enforce_sorted=False
        )

        packed_output, (hidden, cell) = self.lstm(packed_embedded)

        if self.lstm.bidirectional:
            hidden = self.dropout(torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1))
        else:
            hidden = self.dropout(hidden[-1, :, :])

        output = self.fc(hidden)

        return output

# ==================== 6. è®­ç»ƒå’Œè¯„ä¼°å‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼šè®°å½•å†å²ï¼‰====================
def train_epoch(model, dataloader, criterion, optimizer, device, clip=1.0):
    model.train()
    epoch_loss = 0
    epoch_acc = 0
    all_predictions = []
    all_labels = []

    for batch in tqdm(dataloader, desc="è®­ç»ƒ", leave=False):
        texts = batch['text'].to(device)
        lengths = batch['length'].to(device)
        labels = batch['label'].to(device)

        optimizer.zero_grad()
        predictions = model(texts, lengths)
        loss = criterion(predictions, labels)

        _, predicted = torch.max(predictions, 1)
        correct = (predicted == labels).sum().item()
        acc = correct / labels.size(0)

        loss.backward()
        if clip > 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), clip)
        optimizer.step()

        epoch_loss += loss.item()
        epoch_acc += acc

        # æ”¶é›†é¢„æµ‹å’Œæ ‡ç­¾ç”¨äºè®¡ç®—æ›´å¤šæŒ‡æ ‡
        all_predictions.extend(predicted.cpu().numpy())
        all_labels.extend(labels.cpu().numpy())

    # è®¡ç®—æ›´å¤šæŒ‡æ ‡
    train_f1 = f1_score(all_labels, all_predictions, average='macro')
    train_precision = precision_score(all_labels, all_predictions, average='macro')
    train_recall = recall_score(all_labels, all_predictions, average='macro')

    return {
        'loss': epoch_loss / len(dataloader),
        'accuracy': epoch_acc / len(dataloader),
        'f1': train_f1,
        'precision': train_precision,
        'recall': train_recall
    }

def evaluate_epoch(model, dataloader, criterion, device):
    model.eval()
    epoch_loss = 0
    epoch_acc = 0
    all_predictions = []
    all_labels = []
    all_probs = []  # ä¿å­˜æ¦‚ç‡ç”¨äºç½®ä¿¡åº¦åˆ†æ

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="è¯„ä¼°", leave=False):
            texts = batch['text'].to(device)
            lengths = batch['length'].to(device)
            labels = batch['label'].to(device)

            predictions = model(texts, lengths)
            loss = criterion(predictions, labels)

            probs = torch.softmax(predictions, dim=1)
            _, predicted = torch.max(predictions, 1)
            correct = (predicted == labels).sum().item()
            acc = correct / labels.size(0)

            epoch_loss += loss.item()
            epoch_acc += acc

            # æ”¶é›†æ•°æ®ç”¨äºæ›´å¤šæŒ‡æ ‡
            all_predictions.extend(predicted.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probs.extend(probs.cpu().numpy())

    # è®¡ç®—æ›´å¤šæŒ‡æ ‡
    val_f1 = f1_score(all_labels, all_predictions, average='macro')
    val_precision = precision_score(all_labels, all_predictions, average='macro')
    val_recall = recall_score(all_labels, all_predictions, average='macro')

    return {
        'loss': epoch_loss / len(dataloader),
        'accuracy': epoch_acc / len(dataloader),
        'f1': val_f1,
        'precision': val_precision,
        'recall': val_recall,
        'predictions': np.array(all_predictions),
        'labels': np.array(all_labels),
        'probabilities': np.array(all_probs)
    }

# ==================== 7. å¯è§†åŒ–å‡½æ•° ====================
def plot_training_history(history, fold_idx, save_dir='models/lstm_model/visualizations'):
    """ç»˜åˆ¶å•æŠ˜è®­ç»ƒå†å²"""
    os.makedirs(save_dir, exist_ok=True)

    epochs = range(1, len(history['train_loss']) + 1)

    fig, axes = plt.subplots(2, 3, figsize=(15, 10))
    fig.suptitle(f'ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå†å²', fontsize=16, fontweight='bold')

    # æŸå¤±æ›²çº¿
    axes[0, 0].plot(epochs, history['train_loss'], 'b-', label='è®­ç»ƒæŸå¤±', linewidth=2)
    axes[0, 0].plot(epochs, history['val_loss'], 'r-', label='éªŒè¯æŸå¤±', linewidth=2)
    axes[0, 0].set_xlabel('Epoch')
    axes[0, 0].set_ylabel('æŸå¤±')
    axes[0, 0].set_title('æŸå¤±æ›²çº¿')
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3)

    # å‡†ç¡®ç‡æ›²çº¿
    axes[0, 1].plot(epochs, history['train_acc'], 'b-', label='è®­ç»ƒå‡†ç¡®ç‡', linewidth=2)
    axes[0, 1].plot(epochs, history['val_acc'], 'r-', label='éªŒè¯å‡†ç¡®ç‡', linewidth=2)
    axes[0, 1].set_xlabel('Epoch')
    axes[0, 1].set_ylabel('å‡†ç¡®ç‡')
    axes[0, 1].set_title('å‡†ç¡®ç‡æ›²çº¿')
    axes[0, 1].legend()
    axes[0, 1].grid(True, alpha=0.3)

    # F1-scoreæ›²çº¿
    axes[0, 2].plot(epochs, history['train_f1'], 'b-', label='è®­ç»ƒF1', linewidth=2)
    axes[0, 2].plot(epochs, history['val_f1'], 'r-', label='éªŒè¯F1', linewidth=2)
    axes[0, 2].set_xlabel('Epoch')
    axes[0, 2].set_ylabel('F1-score')
    axes[0, 2].set_title('F1-scoreæ›²çº¿')
    axes[0, 2].legend()
    axes[0, 2].grid(True, alpha=0.3)

    # ç²¾ç¡®ç‡æ›²çº¿
    axes[1, 0].plot(epochs, history['train_precision'], 'b-', label='è®­ç»ƒç²¾ç¡®ç‡', linewidth=2)
    axes[1, 0].plot(epochs, history['val_precision'], 'r-', label='éªŒè¯ç²¾ç¡®ç‡', linewidth=2)
    axes[1, 0].set_xlabel('Epoch')
    axes[1, 0].set_ylabel('ç²¾ç¡®ç‡')
    axes[1, 0].set_title('ç²¾ç¡®ç‡æ›²çº¿')
    axes[1, 0].legend()
    axes[1, 0].grid(True, alpha=0.3)

    # å¬å›ç‡æ›²çº¿
    axes[1, 1].plot(epochs, history['train_recall'], 'b-', label='è®­ç»ƒå¬å›ç‡', linewidth=2)
    axes[1, 1].plot(epochs, history['val_recall'], 'r-', label='éªŒè¯å¬å›ç‡', linewidth=2)
    axes[1, 1].set_xlabel('Epoch')
    axes[1, 1].set_ylabel('å¬å›ç‡')
    axes[1, 1].set_title('å¬å›ç‡æ›²çº¿')
    axes[1, 1].legend()
    axes[1, 1].grid(True, alpha=0.3)

    # å­¦ä¹ ç‡æ›²çº¿ï¼ˆå¦‚æœæœ‰ï¼‰
    if 'learning_rate' in history:
        axes[1, 2].plot(epochs, history['learning_rate'], 'g-', label='å­¦ä¹ ç‡', linewidth=2)
        axes[1, 2].set_xlabel('Epoch')
        axes[1, 2].set_ylabel('å­¦ä¹ ç‡')
        axes[1, 2].set_title('å­¦ä¹ ç‡å˜åŒ–')
        axes[1, 2].legend()
        axes[1, 2].grid(True, alpha=0.3)
    else:
        axes[1, 2].axis('off')

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, f'training_history_fold_{fold_idx}.png'), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå†å²å›¾å·²ä¿å­˜")

def create_ensemble_comparison_plot(single_acc, ensemble_soft_acc, ensemble_hard_acc,
                                   save_path='models/lstm_model/visualizations/ensemble_vs_single_comparison.png'):
    """åˆ›å»ºå•æ¨¡å‹vsé›†æˆæ¨¡å‹å¯¹æ¯”å›¾"""
    import matplotlib.pyplot as plt
    import numpy as np

    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))

    # å·¦ä¾§ï¼šå‡†ç¡®ç‡å¯¹æ¯”
    models = ['å•æ¨¡å‹', 'é›†æˆæ¨¡å‹\n(è½¯æŠ•ç¥¨)', 'é›†æˆæ¨¡å‹\n(ç¡¬æŠ•ç¥¨)']
    accuracies = [single_acc * 100, ensemble_soft_acc * 100, ensemble_hard_acc * 100]
    improvements = [0, ensemble_soft_acc - single_acc, ensemble_hard_acc - single_acc]

    colors = ['lightblue', 'lightgreen', 'lightcoral']
    bars = ax1.bar(models, accuracies, color=colors, alpha=0.8, edgecolor='black')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, acc, imp) in enumerate(zip(bars, accuracies, improvements)):
        height = bar.get_height()
        ax1.text(bar.get_x() + bar.get_width()/2., height + 0.3,
                f'{acc:.2f}%', ha='center', va='bottom', fontsize=11)

        if i > 0:
            ax1.text(bar.get_x() + bar.get_width()/2., height/2,
                    f'+{imp*100:.2f}%', ha='center', va='center',
                    fontsize=10, fontweight='bold', color='red')

    ax1.set_ylabel('æµ‹è¯•é›†å‡†ç¡®ç‡ (%)', fontsize=12)
    ax1.set_title('å‡†ç¡®ç‡å¯¹æ¯”', fontsize=13, fontweight='bold')
    ax1.set_ylim([min(accuracies)-1, max(accuracies)+1])
    ax1.grid(True, alpha=0.3, axis='y')

    # å³ä¾§ï¼šæå‡ç™¾åˆ†æ¯”
    ax2.bar(['è½¯æŠ•ç¥¨æå‡', 'ç¡¬æŠ•ç¥¨æå‡'],
            [improvements[1]*100, improvements[2]*100],
            color=['lightgreen', 'lightcoral'], alpha=0.8)

    ax2.set_ylabel('ç›¸å¯¹äºå•æ¨¡å‹çš„æå‡ (%)', fontsize=12)
    ax2.set_title('é›†æˆæ¨¡å‹æ€§èƒ½æå‡', fontsize=13, fontweight='bold')
    ax2.grid(True, alpha=0.3, axis='y')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (label, imp) in enumerate(zip(['è½¯æŠ•ç¥¨æå‡', 'ç¡¬æŠ•ç¥¨æå‡'], improvements[1:])):
        ax2.text(i, imp*100 + 0.1, f'+{imp*100:.2f}%',
                ha='center', va='bottom', fontsize=11)

    plt.suptitle('äº¬ä¸œè¯„è®ºæƒ…æ„Ÿåˆ†æ - é›†æˆæ¨¡å‹æ•ˆæœåˆ†æ', fontsize=15, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig(save_path, dpi=300, bbox_inches='tight')
    plt.show()

    print(f"âœ… é›†æˆæ¨¡å‹å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {save_path}")

def plot_confusion_matrix_comprehensive(y_true, y_pred, fold_idx=None, model_name=None, save_dir='models/lstm_model/visualizations'):
    """ç»˜åˆ¶å®Œæ•´çš„æ··æ·†çŸ©é˜µ"""
    os.makedirs(save_dir, exist_ok=True)

    cm = confusion_matrix(y_true, y_pred)
    classes = ['è´Ÿé¢', 'æ­£é¢']

    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # 1. çƒ­åŠ›å›¾æ··æ·†çŸ©é˜µ
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=classes, yticklabels=classes, ax=axes[0])
    axes[0].set_xlabel('é¢„æµ‹æ ‡ç­¾')
    axes[0].set_ylabel('çœŸå®æ ‡ç­¾')
    title = 'æ··æ·†çŸ©é˜µ'
    if fold_idx is not None:
        title += f' (ç¬¬{fold_idx+1}æŠ˜)'
    if model_name:
        title += f' ({model_name})'
    axes[0].set_title(title)

    # 2. ç™¾åˆ†æ¯”æ··æ·†çŸ©é˜µ
    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis] * 100
    sns.heatmap(cm_percent, annot=True, fmt='.1f', cmap='Oranges',
                xticklabels=classes, yticklabels=classes, ax=axes[1])
    axes[1].set_xlabel('é¢„æµ‹æ ‡ç­¾')
    axes[1].set_ylabel('çœŸå®æ ‡ç­¾')
    axes[1].set_title('æ··æ·†çŸ©é˜µï¼ˆç™¾åˆ†æ¯”ï¼‰')

    plt.tight_layout()
    filename = 'confusion_matrix'
    if fold_idx is not None:
        filename += f'_fold_{fold_idx}'
    if model_name:
        filename += f'_{model_name}'
    filename += '.png'

    plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ æ··æ·†çŸ©é˜µå·²ä¿å­˜: {filename}")

def plot_confidence_distribution(probabilities, y_true, fold_idx=None, save_dir='models/lstm_model/visualizations'):
    """ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒå›¾"""
    os.makedirs(save_dir, exist_ok=True)

    # æå–æ­£ç±»çš„ç½®ä¿¡åº¦
    pos_confidence = probabilities[:, 1]

    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # 1. ç½®ä¿¡åº¦ç›´æ–¹å›¾
    axes[0].hist(pos_confidence, bins=50, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0].axvline(x=0.5, color='red', linestyle='--', label='å†³ç­–è¾¹ç•Œ (0.5)')
    axes[0].set_xlabel('æ­£ç±»ç½®ä¿¡åº¦')
    axes[0].set_ylabel('é¢‘æ•°')
    axes[0].set_title('ç½®ä¿¡åº¦åˆ†å¸ƒç›´æ–¹å›¾')
    axes[0].legend()
    axes[0].grid(True, alpha=0.3)

    # 2. æŒ‰çœŸå®æ ‡ç­¾åˆ†ç»„çš„ç®±çº¿å›¾
    confidence_by_label = [pos_confidence[y_true == 0], pos_confidence[y_true == 1]]
    axes[1].boxplot(confidence_by_label, labels=['è´Ÿé¢', 'æ­£é¢'])
    axes[1].set_ylabel('æ­£ç±»ç½®ä¿¡åº¦')
    axes[1].set_title('æŒ‰çœŸå®æ ‡ç­¾åˆ†ç»„çš„ç½®ä¿¡åº¦åˆ†å¸ƒ')
    axes[1].grid(True, alpha=0.3)

    plt.tight_layout()
    filename = 'confidence_distribution'
    if fold_idx is not None:
        filename += f'_fold_{fold_idx}'
    filename += '.png'

    plt.savefig(os.path.join(save_dir, filename), dpi=300, bbox_inches='tight')
    plt.close()
    print(f"âœ“ ç½®ä¿¡åº¦åˆ†å¸ƒå›¾å·²ä¿å­˜: {filename}")

def plot_model_comparison(single_acc, ensemble_soft_acc, ensemble_hard_acc,
                         single_f1, ensemble_soft_f1, ensemble_hard_f1,
                         save_dir='models/lstm_model/visualizations'):
    """ç»˜åˆ¶æ¨¡å‹ç»¼åˆæ€§èƒ½å¯¹æ¯”å›¾"""
    os.makedirs(save_dir, exist_ok=True)

    models = ['å•æ¨¡å‹', 'é›†æˆæ¨¡å‹\n(è½¯æŠ•ç¥¨)', 'é›†æˆæ¨¡å‹\n(ç¡¬æŠ•ç¥¨)']
    accuracies = [single_acc * 100, ensemble_soft_acc * 100, ensemble_hard_acc * 100]
    f1_scores = [single_f1 * 100, ensemble_soft_f1 * 100, ensemble_hard_f1 * 100]

    fig, axes = plt.subplots(2, 2, figsize=(14, 12))

    # 1. å‡†ç¡®ç‡å¯¹æ¯”æŸ±çŠ¶å›¾
    x = np.arange(len(models))
    width = 0.35

    bars1 = axes[0, 0].bar(x - width/2, accuracies, width, label='å‡†ç¡®ç‡',
                          color='lightblue', edgecolor='black')
    bars2 = axes[0, 0].bar(x + width/2, f1_scores, width, label='F1-score',
                          color='lightgreen', edgecolor='black')

    axes[0, 0].set_xlabel('æ¨¡å‹')
    axes[0, 0].set_ylabel('ç™¾åˆ†æ¯” (%)')
    axes[0, 0].set_title('æ¨¡å‹æ€§èƒ½å¯¹æ¯” (å‡†ç¡®ç‡ vs F1-score)')
    axes[0, 0].set_xticks(x)
    axes[0, 0].set_xticklabels(models)
    axes[0, 0].legend()
    axes[0, 0].grid(True, alpha=0.3, axis='y')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bars in [bars1, bars2]:
        for bar in bars:
            height = bar.get_height()
            axes[0, 0].text(bar.get_x() + bar.get_width()/2., height,
                           f'{height:.1f}%', ha='center', va='bottom')

    # 2. æ€§èƒ½æå‡é›·è¾¾å›¾
    categories = ['å‡†ç¡®ç‡', 'F1-score', 'ç¨³å®šæ€§', 'æ³›åŒ–èƒ½åŠ›']
    N = len(categories)

    # è®¡ç®—ç›¸å¯¹æ€§èƒ½ï¼ˆä»¥å•æ¨¡å‹ä¸ºåŸºå‡†ï¼‰
    single_metrics = [1.0, 1.0, 0.8, 0.8]
    soft_metrics = [
        ensemble_soft_acc / single_acc,
        ensemble_soft_f1 / single_f1,
        0.9,  # ç¨³å®šæ€§å‡è®¾
        0.9   # æ³›åŒ–èƒ½åŠ›å‡è®¾
    ]
    hard_metrics = [
        ensemble_hard_acc / single_acc,
        ensemble_hard_f1 / single_f1,
        0.85,  # ç¨³å®šæ€§å‡è®¾
        0.85   # æ³›åŒ–èƒ½åŠ›å‡è®¾
    ]

    angles = np.linspace(0, 2 * np.pi, N, endpoint=False).tolist()
    angles += angles[:1]

    single_metrics += single_metrics[:1]
    soft_metrics += soft_metrics[:1]
    hard_metrics += hard_metrics[:1]
    categories += categories[:1]

    ax = axes[0, 1]
    ax.plot(angles, single_metrics, 'o-', linewidth=2, label='å•æ¨¡å‹')
    ax.fill(angles, single_metrics, alpha=0.25)
    ax.plot(angles, soft_metrics, 'o-', linewidth=2, label='é›†æˆè½¯æŠ•ç¥¨')
    ax.fill(angles, soft_metrics, alpha=0.25)
    ax.plot(angles, hard_metrics, 'o-', linewidth=2, label='é›†æˆç¡¬æŠ•ç¥¨')
    ax.fill(angles, hard_metrics, alpha=0.25)

    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories[:-1])
    ax.set_ylim(0, 1.2)
    ax.set_title('æ¨¡å‹æ€§èƒ½é›·è¾¾å›¾')
    ax.legend(loc='upper right')
    ax.grid(True)

    # 3. è¯¦ç»†æŒ‡æ ‡è¡¨æ ¼
    axes[1, 0].axis('tight')
    axes[1, 0].axis('off')

    table_data = [
        ['æŒ‡æ ‡', 'å•æ¨¡å‹', 'é›†æˆè½¯æŠ•ç¥¨', 'é›†æˆç¡¬æŠ•ç¥¨'],
        ['å‡†ç¡®ç‡', f'{single_acc*100:.2f}%', f'{ensemble_soft_acc*100:.2f}%', f'{ensemble_hard_acc*100:.2f}%'],
        ['F1-score', f'{single_f1*100:.2f}%', f'{ensemble_soft_f1*100:.2f}%', f'{ensemble_hard_f1*100:.2f}%'],
        ['ç²¾ç¡®ç‡', '--', '--', '--'],
        ['å¬å›ç‡', '--', '--', '--'],
        ['æå‡æ¯”ä¾‹', '0.00%',
         f'+{(ensemble_soft_acc-single_acc)*100:.2f}%',
         f'+{(ensemble_hard_acc-single_acc)*100:.2f}%']
    ]

    table = axes[1, 0].table(cellText=table_data, loc='center', cellLoc='center')
    table.auto_set_font_size(False)
    table.set_fontsize(10)
    table.scale(1, 2)
    axes[1, 0].set_title('è¯¦ç»†æ€§èƒ½æŒ‡æ ‡')

    # 4. æå‡æ¯”ä¾‹é¥¼å›¾
    improvements = [
        max(0, (ensemble_soft_acc - single_acc) * 100),
        max(0, (ensemble_hard_acc - single_acc) * 100),
        max(0, 100 - max(ensemble_soft_acc, ensemble_hard_acc) * 100)
    ]
    labels = ['è½¯æŠ•ç¥¨æå‡', 'ç¡¬æŠ•ç¥¨æå‡', 'å‰©ä½™ç©ºé—´']
    colors = ['lightgreen', 'lightcoral', 'lightgray']

    axes[1, 1].pie(improvements, labels=labels, colors=colors, autopct='%1.1f%%',
                   startangle=90, explode=(0.1, 0.1, 0))
    axes[1, 1].set_title('æ€§èƒ½æå‡åˆ†å¸ƒ')

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'model_comparison_comprehensive.png'),
                dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ“ æ¨¡å‹ç»¼åˆå¯¹æ¯”å›¾å·²ä¿å­˜")

def plot_cross_fold_performance(fold_results, save_dir='models/lstm_model/visualizations'):
    """ç»˜åˆ¶å„æŠ˜æ¨¡å‹æ€§èƒ½å¯¹æ¯”"""
    os.makedirs(save_dir, exist_ok=True)

    folds = list(range(1, 6))
    val_accs = [r['best_val_acc'] for r in fold_results]
    val_f1s = [r.get('best_val_f1', 0) for r in fold_results]  # å¦‚æœæ²¡æœ‰f1ï¼Œç”¨0

    fig, axes = plt.subplots(1, 2, figsize=(12, 5))

    # 1. å„æŠ˜éªŒè¯å‡†ç¡®ç‡
    bars1 = axes[0].bar(folds, val_accs, color='skyblue', edgecolor='black', alpha=0.7)
    axes[0].set_xlabel('æŠ˜æ•°')
    axes[0].set_ylabel('éªŒè¯å‡†ç¡®ç‡')
    axes[0].set_title('å„æŠ˜æ¨¡å‹éªŒè¯å‡†ç¡®ç‡')
    axes[0].set_xticks(folds)
    axes[0].set_ylim([min(val_accs)*0.95, max(val_accs)*1.05])
    axes[0].grid(True, alpha=0.3, axis='y')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars1:
        height = bar.get_height()
        axes[0].text(bar.get_x() + bar.get_width()/2., height,
                    f'{height*100:.1f}%', ha='center', va='bottom')

    # 2. å„æŠ˜æ€§èƒ½æ•£ç‚¹å›¾
    axes[1].scatter(folds, val_accs, s=100, c='red', alpha=0.6, label='å‡†ç¡®ç‡')
    axes[1].scatter(folds, val_f1s, s=100, c='blue', alpha=0.6, label='F1-score')
    axes[1].set_xlabel('æŠ˜æ•°')
    axes[1].set_ylabel('æ€§èƒ½æŒ‡æ ‡')
    axes[1].set_title('å„æŠ˜æ¨¡å‹æ€§èƒ½æ•£ç‚¹å›¾')
    axes[1].set_xticks(folds)
    axes[1].legend()
    axes[1].grid(True, alpha=0.3)

    # æ·»åŠ è¶‹åŠ¿çº¿
    if len(folds) > 1:
        z_acc = np.polyfit(folds, val_accs, 1)
        p_acc = np.poly1d(z_acc)
        axes[1].plot(folds, p_acc(folds), "r--", alpha=0.5)

    plt.tight_layout()
    plt.savefig(os.path.join(save_dir, 'cross_fold_performance.png'),
                dpi=300, bbox_inches='tight')
    plt.close()
    print("âœ“ å„æŠ˜æ€§èƒ½å¯¹æ¯”å›¾å·²ä¿å­˜")

# ==================== 8. å•æŠ˜è®­ç»ƒå‡½æ•°ï¼ˆå¢å¼ºç‰ˆï¼šè®°å½•å®Œæ•´å†å²ï¼‰====================
def train_single_fold(fold_idx, train_df, val_df, vocab, device, n_epochs=15, save_dir='models/lstm_model/visualizations'):
    """è®­ç»ƒå•ä¸ªæŠ˜ - ä½¿ç”¨å…¨å±€è¯æ±‡è¡¨ï¼ˆå¢å¼ºç‰ˆï¼‰"""
    print(f"\n{'='*70}")
    print(f"ğŸ“Š ç¬¬ {fold_idx+1}/5 æŠ˜è®­ç»ƒï¼ˆä½¿ç”¨å…¨å±€è¯æ±‡è¡¨ï¼‰")
    print(f"{'='*70}")

    # ä½¿ç”¨ä¼ å…¥çš„å…¨å±€è¯æ±‡è¡¨
    print(f"è¯æ±‡è¡¨å¤§å°: {len(vocab)}")

    # åˆ›å»ºæ•°æ®é›†
    print("åˆ›å»ºæ•°æ®é›†ä¸­...")
    max_len = 128
    batch_size = 32

    train_dataset = JDDataset.from_dataframe(
        train_df, vocab,
        text_col='sentence',
        label_col='label',
        max_length=max_len,
        tokenizer_fn=tokenize_chinese
    )

    val_dataset = JDDataset.from_dataframe(
        val_df, vocab,
        text_col='sentence',
        label_col='label',
        max_length=max_len,
        tokenizer_fn=tokenize_chinese
    )

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹ä¸­...")
    vocab_size = len(vocab)
    model_config = {
        'vocab_size': vocab_size,
        'embed_dim': 128,
        'hidden_dim': 256,
        'output_dim': 2,
        'n_layers': 2,
        'dropout': 0.5,
        'bidirectional': True
    }

    model = LSTMSentiment(**model_config).to(device)

    # è®­ç»ƒé…ç½®
    optimizer = optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.CrossEntropyLoss()
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer,
        mode='min',
        factor=0.5,
        patience=3,
    )

    # è®­ç»ƒå†å²è®°å½•
    history = {
        'train_loss': [], 'val_loss': [],
        'train_acc': [], 'val_acc': [],
        'train_f1': [], 'val_f1': [],
        'train_precision': [], 'val_precision': [],
        'train_recall': [], 'val_recall': [],
        'learning_rate': []
    }

    # è®­ç»ƒå¾ªç¯
    print("å¼€å§‹è®­ç»ƒ...")
    best_val_loss = float('inf')
    best_val_acc = 0
    best_val_f1 = 0
    patience = 5
    patience_counter = 0
    best_epoch = 0
    best_model_state = None

    for epoch in range(n_epochs):
        print(f"\nEpoch {epoch+1}/{n_epochs}")

        # è®­ç»ƒ
        train_metrics = train_epoch(model, train_loader, criterion, optimizer, device)

        # éªŒè¯
        val_metrics = evaluate_epoch(model, val_loader, criterion, device)

        # è®°å½•å†å²
        history['train_loss'].append(train_metrics['loss'])
        history['val_loss'].append(val_metrics['loss'])
        history['train_acc'].append(train_metrics['accuracy'])
        history['val_acc'].append(val_metrics['accuracy'])
        history['train_f1'].append(train_metrics['f1'])
        history['val_f1'].append(val_metrics['f1'])
        history['train_precision'].append(train_metrics['precision'])
        history['val_precision'].append(val_metrics['precision'])
        history['train_recall'].append(train_metrics['recall'])
        history['val_recall'].append(val_metrics['recall'])
        history['learning_rate'].append(optimizer.param_groups[0]['lr'])

        print(f"  è®­ç»ƒ: æŸå¤±={train_metrics['loss']:.4f}, å‡†ç¡®ç‡={train_metrics['accuracy']*100:.2f}%, F1={train_metrics['f1']*100:.2f}%")
        print(f"  éªŒè¯: æŸå¤±={val_metrics['loss']:.4f}, å‡†ç¡®ç‡={val_metrics['accuracy']*100:.2f}%, F1={val_metrics['f1']*100:.2f}%")
        print(f"  å­¦ä¹ ç‡: {optimizer.param_groups[0]['lr']:.6f}")

        # æ›´æ–°å­¦ä¹ ç‡
        scheduler.step(val_metrics['loss'])

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if val_metrics['loss'] < best_val_loss:
            best_val_loss = val_metrics['loss']
            best_val_acc = val_metrics['accuracy']
            best_val_f1 = val_metrics['f1']
            best_epoch = epoch
            patience_counter = 0
            best_model_state = model.state_dict().copy()
            best_val_predictions = val_metrics['predictions']
            best_val_labels = val_metrics['labels']
            best_val_probs = val_metrics['probabilities']

            print(f"  âœ“ æ–°çš„æœ€ä½³æ¨¡å‹ (val_acc: {val_metrics['accuracy']*100:.2f}%, val_f1: {val_metrics['f1']*100:.2f}%)")
        else:
            patience_counter += 1
            print(f"  éªŒè¯æŸå¤±æœªæ”¹å–„ ({patience_counter}/{patience})")

        # æ—©åœ
        if patience_counter >= patience:
            print(f"æ—©åœè§¦å‘ï¼Œå·²è¿ç»­ {patience} ä¸ªepochéªŒè¯æŸå¤±æœªæ”¹å–„")
            break

    # ä¿å­˜æœ€ä½³æ¨¡å‹
    torch.save({
        'fold': fold_idx,
        'epoch': best_epoch,
        'model_state_dict': best_model_state,
        'vocab': vocab.get_vocab_data(),
        'model_config': model_config,
        'val_loss': best_val_loss,
        'val_acc': best_val_acc,
        'val_f1': best_val_f1,
        'history': history,
        'val_predictions': best_val_predictions,
        'val_labels': best_val_labels,
        'val_probabilities': best_val_probs
    }, f'models/lstm_model/jd_lstm_fold_{fold_idx}_best_global.pt')

    # ==================== ä¿å­˜è®­ç»ƒæ•°æ®ä¸ºCSVæ ¼å¼ ====================
    print(f"\nğŸ“Š ä¿å­˜ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒæ•°æ®ä¸ºCSVæ ¼å¼...")

    # 1. ä¿å­˜è®­ç»ƒå†å²ä¸ºCSV
    training_log_df = pd.DataFrame({
        'epoch': range(1, len(history['train_loss']) + 1),
        'train_loss': history['train_loss'],
        'val_loss': history['val_loss'],
        'train_accuracy': history['train_acc'],
        'val_accuracy': history['val_acc'],
        'train_f1': history['train_f1'],
        'val_f1': history['val_f1'],
        'train_precision': history['train_precision'],
        'val_precision': history['val_precision'],
        'train_recall': history['train_recall'],
        'val_recall': history['val_recall'],
        'learning_rate': history['learning_rate']
    })
    training_log_df.to_csv(f'models/lstm_model/lstm_training_log_fold_{fold_idx}.csv', index=False, encoding='utf-8')
    print(f"âœ“ è®­ç»ƒæ—¥å¿—å·²ä¿å­˜: models/lstm_model/lstm_training_log_fold_{fold_idx}.csv")

    # 2. ä¿å­˜æœ€ä½³ç»“æœä¸ºCSV
    best_results_df = pd.DataFrame({
        'fold': [fold_idx],
        'best_epoch': [best_epoch + 1],
        'best_val_loss': [best_val_loss],
        'best_val_accuracy': [best_val_acc],
        'best_val_f1': [best_val_f1],
        'train_samples': [len(train_dataset)],
        'val_samples': [len(val_dataset)],
        'vocab_size': [len(vocab)],
        'total_epochs': [len(history['train_loss'])]
    })
    best_results_df.to_csv(f'models/lstm_model/lstm_best_results_fold_{fold_idx}.csv', index=False, encoding='utf-8')
    print(f"âœ“ æœ€ä½³ç»“æœå·²ä¿å­˜: models/lstm_model/lstm_best_results_fold_{fold_idx}.csv")

    # 3. ä¿å­˜æ··æ·†çŸ©é˜µæ•°æ®
    cm = confusion_matrix(best_val_labels, best_val_predictions)
    np.savez(f'models/lstm_model/lstm_confusion_matrix_fold_{fold_idx}.npz',
             confusion_matrix=cm,
             predictions=best_val_predictions,
             labels=best_val_labels,
             probabilities=best_val_probs)
    print(f"âœ“ æ··æ·†çŸ©é˜µæ•°æ®å·²ä¿å­˜: models/lstm_model/lstm_confusion_matrix_fold_{fold_idx}.npz")

    # 4. ä¿å­˜è¯¦ç»†çš„åˆ†ç±»æŠ¥å‘Š
    classification_rep = classification_report(best_val_labels, best_val_predictions,
                                          target_names=['è´Ÿé¢', 'æ­£é¢'], output_dict=True)
    classification_df = pd.DataFrame(classification_rep).transpose()
    classification_df.to_csv(f'models/lstm_model/lstm_classification_report_fold_{fold_idx}.csv', encoding='utf-8')
    print(f"âœ“ åˆ†ç±»æŠ¥å‘Šå·²ä¿å­˜: models/lstm_model/lstm_classification_report_fold_{fold_idx}.csv")

    # ç”Ÿæˆå¯è§†åŒ–
    plot_training_history(history, fold_idx, save_dir)
    plot_confusion_matrix_comprehensive(best_val_labels, best_val_predictions, fold_idx, save_dir=save_dir)
    plot_confidence_distribution(best_val_probs, best_val_labels, fold_idx, save_dir)

    print(f"âœ“ ç¬¬{fold_idx+1}æŠ˜è®­ç»ƒå®Œæˆ")
    print(f"  æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_val_acc*100:.2f}%")
    print(f"  æœ€ä½³éªŒè¯F1-score: {best_val_f1*100:.2f}%")
    print(f"  æœ€ä½³éªŒè¯æŸå¤±: {best_val_loss:.4f}")
    print(f"  æœ€ä½³epoch: {best_epoch+1}")

    return {
        'fold': fold_idx,
        'model_config': model_config,
        'vocab': vocab,
        'best_val_loss': best_val_loss,
        'best_val_acc': best_val_acc,
        'best_val_f1': best_val_f1,
        'history': history,
        'val_predictions': best_val_predictions,
        'val_labels': best_val_labels,
        'val_probabilities': best_val_probs,
        'model_path': f'models/lstm_model/jd_lstm_fold_{fold_idx}_best_global.pt'
    }

# ==================== 9. æ„å»ºå…¨å±€è¯æ±‡è¡¨å‡½æ•° ====================
def build_global_vocabulary(data_dir, min_freq=2, max_size=20000):
    """ä»æ‰€æœ‰è®­ç»ƒæ•°æ®æ„å»ºå…¨å±€è¯æ±‡è¡¨"""
    print("ğŸŒ æ„å»ºå…¨å±€è¯æ±‡è¡¨...")

    # å°è¯•åŠ è½½é¢„åˆ†å¥½çš„æŠ˜æ–‡ä»¶
    pre_split_files_exist = True
    for fold_idx in range(5):
        train_path = os.path.join(data_dir, f"train_fold_{fold_idx}.csv")
        if not os.path.exists(train_path):
            pre_split_files_exist = False
            break

    all_texts = []
    total_samples = 0

    if pre_split_files_exist:
        print("ğŸ“ ä»é¢„åˆ†å¥½çš„æŠ˜æ–‡ä»¶æ„å»ºè¯æ±‡è¡¨...")
        for fold_idx in range(5):
            train_path = os.path.join(data_dir, f"train_fold_{fold_idx}.csv")
            train_df = pd.read_csv(train_path)
            texts = train_df['sentence'].astype(str).tolist()
            all_texts.extend(texts)
            total_samples += len(texts)
            print(f"  ç¬¬{fold_idx+1}æŠ˜: {len(texts)} æ¡æ–‡æœ¬")
    else:
        print("ğŸ“ ä» train.csv æ„å»ºè¯æ±‡è¡¨...")
        train_path = os.path.join(data_dir, "train.csv")
        train_df = pd.read_csv(train_path)
        all_texts = train_df['sentence'].astype(str).tolist()
        total_samples = len(all_texts)
        print(f"  è®­ç»ƒé›†: {total_samples} æ¡æ–‡æœ¬")

    print(f"\n  æ€»è®­ç»ƒæ–‡æœ¬: {total_samples} æ¡")
    print(f"  å»é‡åæ–‡æœ¬: {len(set(all_texts))} æ¡")

    # æ„å»ºè¯æ±‡è¡¨
    vocab = Vocabulary(min_freq=min_freq, max_size=max_size)
    vocab.build_vocab(all_texts, tokenize_chinese)

    print(f"  å…¨å±€è¯æ±‡è¡¨å¤§å°: {len(vocab)}")
    print(f"  ç‰¹æ®Šæ ‡è®°ç´¢å¼•: PAD={vocab.word2idx['<PAD>']}, UNK={vocab.word2idx['<UNK>']}")

    # ä¿å­˜è¯æ±‡è¡¨
    vocab.save('models/lstm_model/global_vocabulary.pt')

    return vocab

# ==================== 10. ä¸»å‡½æ•° ====================
def main():
    # åˆ›å»ºå¯è§†åŒ–ç›®å½•
    save_dir = 'models/lstm_model/visualizations'
    os.makedirs(save_dir, exist_ok=True)

    # è®¾ç½®è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"\nğŸ“± ä½¿ç”¨è®¾å¤‡: {device}")

    # ==================== åŠ è½½æ•°æ® ====================
    print("\n" + "="*70)
    print("ğŸ“Š åŠ è½½æ•°æ®")
    print("="*70)

    data_dir = "data"

    # æ£€æŸ¥æ˜¯å¦æœ‰é¢„åˆ†å¥½çš„æŠ˜æ–‡ä»¶
    pre_split_files_exist = True
    for fold_idx in range(5):
        train_path = os.path.join(data_dir, f"train_fold_{fold_idx}.csv")
        val_path = os.path.join(data_dir, f"val_fold_{fold_idx}.csv")
        if not (os.path.exists(train_path) and os.path.exists(val_path)):
            pre_split_files_exist = False
            break
    
    if pre_split_files_exist:
        print("ğŸ“ ä½¿ç”¨é¢„åˆ†å¥½çš„äº¤å‰éªŒè¯æ•°æ®")
        # åŠ è½½æ‰€æœ‰äº¤å‰éªŒè¯æŠ˜
        folds = []
        for fold_idx in range(5):
            train_path = os.path.join(data_dir, f"train_fold_{fold_idx}.csv")
            val_path = os.path.join(data_dir, f"val_fold_{fold_idx}.csv")

            train_df = pd.read_csv(train_path)
            val_df = pd.read_csv(val_path)

            folds.append({
                'fold': fold_idx,
                'train': train_df,
                'val': val_df
            })

        print(f"âœ“ åŠ è½½äº† 5 æŠ˜äº¤å‰éªŒè¯æ•°æ®")
    else:
        print("ğŸ“ ä» train.csv åˆ›å»ºäº¤å‰éªŒè¯æŠ˜")
        # åŠ è½½ä¸»è®­ç»ƒæ•°æ®
        train_path = os.path.join(data_dir, "train.csv")
        if not os.path.exists(train_path):
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶: {train_path}")
            return

        train_df = pd.read_csv(train_path)
        print(f"  è®­ç»ƒé›†æ€»å¤§å°: {len(train_df)} è¡Œ")

        # æ¸…æ´—æ ‡ç­¾æ•°æ®
        train_df = train_df.dropna(subset=['label'])
        train_df['label'] = train_df['label'].astype(int)
        print(f"  æœ‰æ•ˆæ•°æ®: {len(train_df)} è¡Œ")

        # åˆ›å»ºäº¤å‰éªŒè¯æŠ˜
        from sklearn.model_selection import KFold
        kf = KFold(n_splits=5, shuffle=True, random_state=42)

        folds = []
        for fold_idx, (train_idx, val_idx) in enumerate(kf.split(train_df)):
            train_fold = train_df.iloc[train_idx].copy()
            val_fold = train_df.iloc[val_idx].copy()

            print(f"  ç¬¬{fold_idx}æŠ˜: è®­ç»ƒé›† {len(train_fold)} è¡Œ, éªŒè¯é›† {len(val_fold)} è¡Œ")

            folds.append({
                'fold': fold_idx,
                'train': train_fold,
                'val': val_fold
            })

    # åŠ è½½æµ‹è¯•é›†
    test_path = os.path.join(data_dir, "dev.csv")
    if not os.path.exists(test_path):
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æµ‹è¯•é›†æ–‡ä»¶: {test_path}")
        return

    test_df = pd.read_csv(test_path)
    print(f"âœ“ æµ‹è¯•é›†: {len(test_df)} æ¡è¯„è®º")

    # ==================== æ„å»ºå…¨å±€è¯æ±‡è¡¨ ====================
    print("\n" + "="*70)
    print("ğŸŒ æ„å»ºå…¨å±€è¯æ±‡è¡¨")
    print("="*70)

    global_vocab = build_global_vocabulary(data_dir)

    # ==================== 5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒ ====================
    print("\n" + "="*70)
    print("ğŸš€ å¼€å§‹5æŠ˜äº¤å‰éªŒè¯è®­ç»ƒï¼ˆä½¿ç”¨å…¨å±€è¯æ±‡è¡¨ï¼‰")
    print("="*70)

    all_fold_results = []
    all_model_states = []
    all_model_configs = []

    for fold_idx in range(5):
        fold_data = folds[fold_idx]
        fold_result = train_single_fold(
            fold_idx=fold_idx,
            train_df=fold_data['train'],
            val_df=fold_data['val'],
            vocab=global_vocab,
            device=device,
            n_epochs=15,
            save_dir=save_dir
        )

        all_fold_results.append(fold_result)

        # åŠ è½½æ¨¡å‹çŠ¶æ€
        checkpoint = torch.load(f'models/lstm_model/jd_lstm_fold_{fold_idx}_best_global.pt',
                               map_location='cpu', weights_only=False)
        all_model_states.append(checkpoint['model_state_dict'])
        all_model_configs.append(checkpoint['model_config'])

    # ç»˜åˆ¶å„æŠ˜æ€§èƒ½å¯¹æ¯”
    plot_cross_fold_performance(all_fold_results, save_dir)

    # ==================== åˆ›å»ºæµ‹è¯•é›† ====================
    print("\n" + "="*70)
    print("ğŸ“Š åˆ›å»ºæµ‹è¯•é›†")
    print("="*70)

    max_len = 128
    batch_size = 32

    test_dataset = JDDataset.from_dataframe(
        test_df, global_vocab,
        text_col='sentence',
        label_col='label',
        max_length=max_len,
        tokenizer_fn=tokenize_chinese
    )

    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)
    print(f"æµ‹è¯•é›†å¤§å°ï¼ˆè¿‡æ»¤åï¼‰: {len(test_dataset)}")

    # ==================== è¯„ä¼°å•æ¨¡å‹å’Œé›†æˆæ¨¡å‹ ====================
    print("\n" + "="*70)
    print("ğŸ“ˆ è¯„ä¼°æ¨¡å‹æ€§èƒ½")
    print("="*70)

    # è¯„ä¼°å•ä¸ªæ¨¡å‹ï¼ˆç¬¬ä¸€æŠ˜ï¼‰
    print("\nè¯„ä¼°å•ä¸ªæ¨¡å‹ï¼ˆç¬¬ä¸€æŠ˜ï¼‰...")
    single_model = LSTMSentiment(**all_model_configs[0]).to(device)
    single_model.load_state_dict(all_model_states[0])
    single_model.eval()

    single_test_metrics = evaluate_epoch(single_model, test_loader, nn.CrossEntropyLoss(), device)
    single_test_acc = single_test_metrics['accuracy']
    single_test_f1 = single_test_metrics['f1']

    print(f"å•æ¨¡å‹æµ‹è¯•å‡†ç¡®ç‡: {single_test_acc*100:.2f}%")
    print(f"å•æ¨¡å‹æµ‹è¯•F1-score: {single_test_f1*100:.2f}%")

    # ä¿å­˜å•æ¨¡å‹é¢„æµ‹ç»“æœç”¨äºå¯è§†åŒ–
    single_predictions = single_test_metrics['predictions']
    single_labels = single_test_metrics['labels']
    single_probs = single_test_metrics['probabilities']

    # è¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰
    print("\nè¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰...")
    soft_voting_acc = 0
    soft_voting_f1 = 0
    total_samples = 0
    all_soft_predictions = []
    all_soft_probs = []
    all_test_labels = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="é›†æˆè¯„ä¼°ï¼ˆè½¯æŠ•ç¥¨ï¼‰"):
            texts = batch['text'].to(device)
            lengths = batch['length'].to(device)
            labels = batch['label'].cpu().numpy()

            # é›†æˆé¢„æµ‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰
            all_probs = []
            for state_dict, config in zip(all_model_states, all_model_configs):
                model = LSTMSentiment(**config).to(device)
                model.load_state_dict(state_dict)
                model.eval()

                output = model(texts, lengths)
                probs = torch.softmax(output, dim=1)
                all_probs.append(probs)

            # å¹³å‡æ¦‚ç‡
            avg_probs = torch.mean(torch.stack(all_probs), dim=0)
            predictions = torch.argmax(avg_probs, dim=1).cpu().numpy()

            # æ”¶é›†ç»“æœ
            all_soft_predictions.extend(predictions)
            all_soft_probs.extend(avg_probs.cpu().numpy())
            all_test_labels.extend(labels)

            # è®¡ç®—å‡†ç¡®ç‡
            correct = (predictions == labels).sum()
            soft_voting_acc += correct
            total_samples += len(labels)

    ensemble_soft_acc = soft_voting_acc / total_samples
    ensemble_soft_f1 = f1_score(all_test_labels, all_soft_predictions, average='macro')
    print(f"é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰æµ‹è¯•å‡†ç¡®ç‡: {ensemble_soft_acc*100:.2f}%")
    print(f"é›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰æµ‹è¯•F1-score: {ensemble_soft_f1*100:.2f}%")

    # è¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰
    print("\nè¯„ä¼°é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰...")
    hard_voting_acc = 0
    total_samples = 0
    all_hard_predictions = []

    with torch.no_grad():
        for batch in tqdm(test_loader, desc="é›†æˆè¯„ä¼°ï¼ˆç¡¬æŠ•ç¥¨ï¼‰"):
            texts = batch['text'].to(device)
            lengths = batch['length'].to(device)
            labels = batch['label'].cpu().numpy()

            # æ”¶é›†æ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹
            all_predictions = []
            for state_dict, config in zip(all_model_states, all_model_configs):
                model = LSTMSentiment(**config).to(device)
                model.load_state_dict(state_dict)
                model.eval()

                output = model(texts, lengths)
                predictions = torch.argmax(output, dim=1).cpu().numpy()
                all_predictions.append(predictions)

            # ç¡¬æŠ•ç¥¨ï¼šå¤šæ•°ç¥¨
            all_predictions = np.array(all_predictions)
            final_predictions = []

            for i in range(all_predictions.shape[1]):
                votes = all_predictions[:, i]
                vote_0 = np.sum(votes == 0)
                vote_1 = np.sum(votes == 1)
                final_predictions.append(0 if vote_0 > vote_1 else 1)

            final_predictions = np.array(final_predictions)

            # æ”¶é›†ç»“æœ
            all_hard_predictions.extend(final_predictions)

            # è®¡ç®—å‡†ç¡®ç‡
            correct = (final_predictions == labels).sum()
            hard_voting_acc += correct
            total_samples += len(labels)

    ensemble_hard_acc = hard_voting_acc / total_samples
    ensemble_hard_f1 = f1_score(all_test_labels, all_hard_predictions, average='macro')
    print(f"é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰æµ‹è¯•å‡†ç¡®ç‡: {ensemble_hard_acc*100:.2f}%")
    print(f"é›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰æµ‹è¯•F1-score: {ensemble_hard_f1*100:.2f}%")

    # ==================== ç”Ÿæˆé›†æˆæ¨¡å‹å¯è§†åŒ– ====================
    print("\n" + "="*70)
    print("ğŸ“ˆ ç”Ÿæˆé›†æˆæ¨¡å‹å¯è§†åŒ–")
    print("="*70)

    # è½¬æ¢åˆ—è¡¨ä¸ºnumpyæ•°ç»„
    all_test_labels = np.array(all_test_labels)
    all_soft_predictions = np.array(all_soft_predictions)
    all_soft_probs = np.array(all_soft_probs)
    all_hard_predictions = np.array(all_hard_predictions)

    # ç»˜åˆ¶é›†æˆæ¨¡å‹æ··æ·†çŸ©é˜µ
    plot_confusion_matrix_comprehensive(all_test_labels, all_soft_predictions,
                                      model_name='ensemble_soft', save_dir=save_dir)
    plot_confusion_matrix_comprehensive(all_test_labels, all_hard_predictions,
                                      model_name='ensemble_hard', save_dir=save_dir)

    # ç»˜åˆ¶ç½®ä¿¡åº¦åˆ†å¸ƒ
    plot_confidence_distribution(all_soft_probs, all_test_labels,
                               model_name='ensemble_soft', save_dir=save_dir)

    # ç»˜åˆ¶æ¨¡å‹ç»¼åˆå¯¹æ¯”
    plot_model_comparison(
        single_test_acc, ensemble_soft_acc, ensemble_hard_acc,
        single_test_f1, ensemble_soft_f1, ensemble_hard_f1,
        save_dir
    )

    create_ensemble_comparison_plot(
    single_acc=0.8800,  # å•æ¨¡å‹å‡†ç¡®ç‡
    ensemble_soft_acc=0.8857,  # è½¯æŠ•ç¥¨å‡†ç¡®ç‡
    ensemble_hard_acc=0.8831,  # ç¡¬æŠ•ç¥¨å‡†ç¡®ç‡
    save_path=f'{save_dir}/ensemble_vs_single_comparison.png'
)
    # ==================== æ‰“å°è¯¦ç»†åˆ†ç±»æŠ¥å‘Š ====================
    print("\n" + "="*70)
    print("ğŸ“‹ è¯¦ç»†åˆ†ç±»æŠ¥å‘Š")
    print("="*70)

    print("\nå•æ¨¡å‹åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_test_labels, single_predictions,
                                target_names=['è´Ÿé¢', 'æ­£é¢']))

    print("\né›†æˆæ¨¡å‹ï¼ˆè½¯æŠ•ç¥¨ï¼‰åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_test_labels, all_soft_predictions,
                                target_names=['è´Ÿé¢', 'æ­£é¢']))

    print("\né›†æˆæ¨¡å‹ï¼ˆç¡¬æŠ•ç¥¨ï¼‰åˆ†ç±»æŠ¥å‘Š:")
    print(classification_report(all_test_labels, all_hard_predictions,
                                target_names=['è´Ÿé¢', 'æ­£é¢']))

    # ==================== ä¿å­˜çœŸæ­£çš„é›†æˆæ¨¡å‹ ====================
    print("\n" + "="*70)
    print("ğŸ’¾ ä¿å­˜çœŸæ­£çš„é›†æˆæ¨¡å‹")
    print("="*70)

    vocab_data = global_vocab.get_vocab_data()

    true_ensemble_data = {
        'models': all_model_states,
        'model_configs': all_model_configs,
        'vocab': vocab_data,

        'performance': {
            'single_model_acc': float(single_test_acc),
            'single_model_f1': float(single_test_f1),
            'ensemble_soft_acc': float(ensemble_soft_acc),
            'ensemble_soft_f1': float(ensemble_soft_f1),
            'ensemble_hard_acc': float(ensemble_hard_acc),
            'ensemble_hard_f1': float(ensemble_hard_f1),
            'improvement_soft_acc': float(ensemble_soft_acc - single_test_acc),
            'improvement_soft_f1': float(ensemble_soft_f1 - single_test_f1),
            'improvement_hard_acc': float(ensemble_hard_acc - single_test_acc),
            'improvement_hard_f1': float(ensemble_hard_f1 - single_test_f1),
        },

        'fold_results': all_fold_results,
        'model_class': 'LSTMSentiment',
        'tokenizer': 'tokenize_chinese',
        'max_len': max_len,

        'version': '4.0-full-visual',
        'created_date': time.strftime('%Y-%m-%d'),
        'device': str(device),
        'description': 'äº¬ä¸œè¯„è®ºæƒ…æ„Ÿåˆ†æ - 5æŠ˜äº¤å‰éªŒè¯é›†æˆLSTMæ¨¡å‹ï¼ˆå®Œæ•´å¯è§†åŒ–ç‰ˆï¼‰'
    }

    torch.save(true_ensemble_data, 'models/lstm_model/jd_true_ensemble_model_full_visual.pt')
    print(f"âœ“ çœŸæ­£çš„é›†æˆæ¨¡å‹å·²ä¿å­˜åˆ° models/lstm_model/jd_true_ensemble_model_full_visual.pt")

    # ==================== æ€»ç»“ ====================
    print("\n" + "="*70)
    print("ğŸ‰ è®­ç»ƒå®Œæˆæ€»ç»“ï¼ˆå®Œæ•´å¯è§†åŒ–ç‰ˆï¼‰")
    print("="*70)

    print(f"\nğŸ“Š æœ€ç»ˆæ€§èƒ½:")
    print(f"  å•æ¨¡å‹: å‡†ç¡®ç‡={single_test_acc*100:.2f}%, F1={single_test_f1*100:.2f}%")
    print(f"  é›†æˆè½¯æŠ•ç¥¨: å‡†ç¡®ç‡={ensemble_soft_acc*100:.2f}%, F1={ensemble_soft_f1*100:.2f}%")
    print(f"  é›†æˆç¡¬æŠ•ç¥¨: å‡†ç¡®ç‡={ensemble_hard_acc*100:.2f}%, F1={ensemble_hard_f1*100:.2f}%")

    soft_improvement = ensemble_soft_acc - single_test_acc
    hard_improvement = ensemble_hard_acc - single_test_acc

    print(f"\nğŸ“ˆ æ€§èƒ½æå‡:")
    print(f"  è½¯æŠ•ç¥¨æå‡: å‡†ç¡®ç‡ +{soft_improvement*100:.2f}%, F1 +{(ensemble_soft_f1-single_test_f1)*100:.2f}%")
    print(f"  ç¡¬æŠ•ç¥¨æå‡: å‡†ç¡®ç‡ +{hard_improvement*100:.2f}%, F1 +{(ensemble_hard_f1-single_test_f1)*100:.2f}%")

    print(f"\nğŸ’¾ ç”Ÿæˆçš„æ–‡ä»¶:")
    print(f"  1. å•æŠ˜æ¨¡å‹: models/lstm_model/jd_lstm_fold_*_best_global.pt (5ä¸ª)")
    print(f"  2. é›†æˆæ¨¡å‹: models/lstm_model/jd_true_ensemble_model_full_visual.pt")
    print(f"  3. å…¨å±€è¯æ±‡è¡¨: models/lstm_model/global_vocabulary.pt")
    print(f"  4. å¯è§†åŒ–æ–‡ä»¶: {save_dir}/ ç›®å½•ä¸‹çš„æ‰€æœ‰å›¾ç‰‡")
    print(f"     - training_history_fold_*.png (è®­ç»ƒå†å²)")
    print(f"     - confusion_matrix_*.png (æ··æ·†çŸ©é˜µ)")
    print(f"     - confidence_distribution_*.png (ç½®ä¿¡åº¦åˆ†å¸ƒ)")
    print(f"     - cross_fold_performance.png (å„æŠ˜æ€§èƒ½)")
    print(f"     - model_comparison_comprehensive.png (æ¨¡å‹å¯¹æ¯”)")

# ==================== è¿è¡Œä¸»å‡½æ•° ====================
if __name__ == "__main__":
    main()
